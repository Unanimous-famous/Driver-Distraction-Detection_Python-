{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkVlysH7X09l"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su9LU_5EX7zE",
        "outputId": "70deb503-2edb-44f6-f0cf-1b03eaf5c7de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to access the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwqMS-lyBuGm",
        "outputId": "c98470f0-5d09-45a7-b6e6-90a291aec06d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XGF1jNBUqxT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29aEc6mLjv0I"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import BatchNormalization, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Reshape, TimeDistributed, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import inception_v3\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "# Set up paths\n",
        "train_faces_path = '/content/drive/MyDrive/AUC/F/train_faces.npy'\n",
        "val_faces_path = '/content/drive/MyDrive/AUC/F/val_faces.npy'\n",
        "test_faces_path = '/content/drive/MyDrive/AUC/F/test_faces.npy'\n",
        "\n",
        "train_metadata_path = '/content/drive/MyDrive/AUC/F/train_metadata.csv'\n",
        "val_metadata_path = '/content/drive/MyDrive/AUC/F/val_metadata.csv'\n",
        "test_metadata_path = '/content/drive/MyDrive/AUC/F/test_metadata.csv'\n",
        "\n",
        "fer_model_path = '/content/drive/My Drive/HHHIncpV3BiLSTM_KDEF.h5'\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/AUC/WV/train'\n",
        "val_dir = '/content/drive/MyDrive/AUC/WV/val'\n",
        "test_dir = '/content/drive/MyDrive/AUC/WV/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v_kIIFuTX-ch"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Load the datasets ---\n",
        "train_faces = np.load(train_faces_path)\n",
        "val_faces = np.load(val_faces_path)\n",
        "test_faces = np.load(test_faces_path)\n",
        "train_metadata_df = pd.read_csv(train_metadata_path)\n",
        "val_metadata_df = pd.read_csv(val_metadata_path)\n",
        "test_metadata_df = pd.read_csv(test_metadata_path)\n",
        "\n",
        "# Extract face detection flags\n",
        "train_face_detected = train_metadata_df['face_detected'].astype(int).to_numpy()\n",
        "val_face_detected = val_metadata_df['face_detected'].astype(int).to_numpy()\n",
        "test_face_detected = test_metadata_df['face_detected'].astype(int).to_numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "OPchuOzNpNtP",
        "outputId": "0b397db0-5fe1-4132-eb02-6d9ecdbd4ea0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'for i, layer in enumerate(fer_base_model.layers):\\n    print(f\"Layer {i}: {layer.name}\")\\n'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the pre-trained FER model\n",
        "fer_base_model = load_model(fer_model_path)\n",
        "\n",
        "# List all layers in the model\n",
        "'''for i, layer in enumerate(fer_base_model.layers):\n",
        "    print(f\"Layer {i}: {layer.name}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W82wTQZ3PGNd"
      },
      "outputs": [],
      "source": [
        "fer_base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPU63OKqby38"
      },
      "outputs": [],
      "source": [
        "# --- FER Feature Extraction ---\n",
        "train_fer_features_output_path = '/content/drive/MyDrive/AUC/F5/train_fer_features.npy'\n",
        "val_fer_features_output_path = '/content/drive/MyDrive/AUC/F5/val_fer_features.npy'\n",
        "test_fer_features_output_path = '/content/drive/MyDrive/AUC/F5/test_fer_features.npy'\n",
        "\n",
        "def extract_fer_features_with_flag(extractor, faces, face_detected, save_path):\n",
        "    features = []\n",
        "    extracted_count = 0\n",
        "    zero_vector_count = 0\n",
        "    for i, face in enumerate(faces):\n",
        "        if face_detected[i] == 1:\n",
        "            feature = extractor.predict(np.expand_dims(face, axis=0))[0]\n",
        "            extracted_count += 1\n",
        "        else:\n",
        "            feature = np.zeros(extractor.output.shape[1])  # Use zeros for no detected face\n",
        "            zero_vector_count += 1\n",
        "        features.append(feature)\n",
        "    features = np.array(features)\n",
        "    np.save(save_path, features)\n",
        "    print(f\"Features extracted: {extracted_count}, Zero vectors used: {zero_vector_count}\")\n",
        "    print(f\"Shape of extracted features: {features.shape}\")\n",
        "    return features\n",
        "# Load or extract and save FER features with flags\n",
        "if not os.path.exists(train_fer_features_output_path):\n",
        "    fer_base_model = load_model(fer_model_path)\n",
        "    feature_extraction_layer = fer_base_model.get_layer(\"dense_7\")\n",
        "    fer_feature_extractor = Model(inputs=fer_base_model.input, outputs=feature_extraction_layer.output)\n",
        "    train_fer_features = extract_fer_features_with_flag(fer_feature_extractor, train_faces, train_face_detected, train_fer_features_output_path)\n",
        "else:\n",
        "    train_fer_features = np.load(train_fer_features_output_path)\n",
        "\n",
        "if not os.path.exists(val_fer_features_output_path):\n",
        "    val_fer_features = extract_fer_features_with_flag(fer_feature_extractor, val_faces, val_face_detected, val_fer_features_output_path)\n",
        "else:\n",
        "    val_fer_features = np.load(val_fer_features_output_path)\n",
        "\n",
        "if not os.path.exists(test_fer_features_output_path):\n",
        "    test_fer_features = extract_fer_features_with_flag(fer_feature_extractor, test_faces, test_face_detected, test_fer_features_output_path)\n",
        "else:\n",
        "    test_fer_features = np.load(test_fer_features_output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDEZwYcp5wP9",
        "outputId": "8769fae9-9579-476f-f064-f7ced26d18d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set - Total: 8934, Zero vectors: 0, Detected faces: 8934\n",
            "Validation set - Total: 961, Zero vectors: 0, Detected faces: 961\n",
            "Test set - Total: 1086, Zero vectors: 0, Detected faces: 1086\n"
          ]
        }
      ],
      "source": [
        "def verify_features(features, face_detected, dataset_name):\n",
        "    num_zero_vectors = np.sum([np.all(f == 0) for f in features])\n",
        "    num_features = features.shape[0]\n",
        "    print(f\"{dataset_name} - Total: {num_features}, Zero vectors: {num_zero_vectors}, Detected faces: {np.sum(face_detected)}\")\n",
        "\n",
        "# Verify extracted features for each dataset\n",
        "verify_features(train_fer_features, train_face_detected, \"Train set\")\n",
        "verify_features(val_fer_features, val_face_detected, \"Validation set\")\n",
        "verify_features(test_fer_features, test_face_detected, \"Test set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0-ev8hFjnQry"
      },
      "outputs": [],
      "source": [
        "# --- FER Feature Loading (Load Only) ---\n",
        "train_fer_features = np.load('/content/drive/MyDrive/AUC/F5/train_fer_features.npy')\n",
        "val_fer_features = np.load('/content/drive/MyDrive/AUC/F5/val_fer_features.npy')\n",
        "test_fer_features = np.load('/content/drive/MyDrive/AUC/F5/test_fer_features.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG68PyhmKLKy",
        "outputId": "498d5078-c071-4f86-dfd7-1623c6d49644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train FER features shape: (8934, 512)\n",
            "Validation FER features shape: (961, 512)\n",
            "Test FER features shape: (1086, 512)\n",
            "Number of training samples (FER features): 8934\n",
            "Number of validation samples (FER features): 961\n",
            "Number of test samples (FER features): 1086\n",
            "Train set - Total: 8934, Zero vectors: 0, Detected faces: 8934\n",
            "Validation set - Total: 961, Zero vectors: 0, Detected faces: 961\n",
            "Test set - Total: 1086, Zero vectors: 0, Detected faces: 1086\n"
          ]
        }
      ],
      "source": [
        "# Print the shape and length of the loaded FER features\n",
        "print(f\"Train FER features shape: {train_fer_features.shape}\")\n",
        "print(f\"Validation FER features shape: {val_fer_features.shape}\")\n",
        "print(f\"Test FER features shape: {test_fer_features.shape}\")\n",
        "\n",
        "# Print the number of samples for FER features\n",
        "print(f\"Number of training samples (FER features): {len(train_fer_features)}\")\n",
        "print(f\"Number of validation samples (FER features): {len(val_fer_features)}\")\n",
        "print(f\"Number of test samples (FER features): {len(test_fer_features)}\")\n",
        "\n",
        "# --- Verification and Summary ---\n",
        "def verify_features(features, face_detected, dataset_name):\n",
        "    num_zero_vectors = np.sum([np.all(f == 0) for f in features])\n",
        "    num_features = features.shape[0]\n",
        "    print(f\"{dataset_name} - Total: {num_features}, Zero vectors: {num_zero_vectors}, Detected faces: {np.sum(face_detected)}\")\n",
        "\n",
        "# Verify extracted features for each dataset\n",
        "verify_features(train_fer_features, train_face_detected, \"Train set\")\n",
        "verify_features(val_fer_features, val_face_detected, \"Validation set\")\n",
        "verify_features(test_fer_features, test_face_detected, \"Test set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XBCoz3JRoxSr"
      },
      "outputs": [],
      "source": [
        "# --- Pose Feature Extraction (Save Only) ---\n",
        "\n",
        "# Define output paths for each subset\n",
        "train_pose_features_output_path = '/content/drive/MyDrive/AUC/F6/train_pose_features.npy'\n",
        "val_pose_features_output_path = '/content/drive/MyDrive/AUC/F6/val_pose_features.npy'\n",
        "test_pose_features_output_path = '/content/drive/MyDrive/AUC/F6/test_pose_features.npy'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCy4kBKGtAQC",
        "outputId": "4e1ef998-7625-4fb8-a055-3e46cb5d03a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        }
      ],
      "source": [
        "####with visibility:\n",
        "def extract_pose_landmarks(image_path):\n",
        "    # Open image and convert to uint8\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = np.array(image)\n",
        "    image_uint8 = image.astype(np.uint8)\n",
        "    results = pose.process(image_uint8)\n",
        "    if results.pose_landmarks:\n",
        "        # Extract the first 23 points (0-22) with x, y, z coordinates and visibility\n",
        "        landmarks = []\n",
        "        for landmark in results.pose_landmarks.landmark[:23]:\n",
        "            landmarks.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "        return landmarks\n",
        "    else:\n",
        "        # Use zeros for no pose landmarks detected (including visibility)\n",
        "        return [0] * (23 * 4)  # 23 landmarks * 4 features each (x, y, z, visibility)\n",
        "\n",
        "# Process and save training set pose features\n",
        "if not os.path.exists(train_pose_features_output_path):\n",
        "    import mediapipe as mp\n",
        "    from PIL import Image\n",
        "\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose()\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(train_dir)\n",
        "    train_pose_features = [extract_pose_landmarks(path) for path, _ in train_dataset.imgs]\n",
        "    np.save(train_pose_features_output_path, train_pose_features)\n",
        "\n",
        "# Process and save validation set pose features\n",
        "if not os.path.exists(val_pose_features_output_path):\n",
        "    val_dataset = datasets.ImageFolder(val_dir)\n",
        "    val_pose_features = [extract_pose_landmarks(path) for path, _ in val_dataset.imgs]\n",
        "    np.save(val_pose_features_output_path, val_pose_features)\n",
        "\n",
        "# Process and save test set pose features\n",
        "if not os.path.exists(test_pose_features_output_path):\n",
        "    test_dataset = datasets.ImageFolder(test_dir)\n",
        "    test_pose_features = [extract_pose_landmarks(path) for path, _ in test_dataset.imgs]\n",
        "    np.save(test_pose_features_output_path, test_pose_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgcWti_aJQJp",
        "outputId": "5a9aea87-2b88-42af-b52e-104d3dc7aaeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Pose features shape: (9538, 92)\n",
            "Validation Pose features shape: (1017, 92)\n",
            "Test Pose features shape: (1123, 92)\n",
            "Number of training samples (Pose features): 9538\n",
            "Number of validation samples (Pose features): 1017\n",
            "Number of test samples (Pose features): 1123\n"
          ]
        }
      ],
      "source": [
        "# --- Pose Feature Loading (Load Only) ---\n",
        "train_pose_features = np.load(train_pose_features_output_path)\n",
        "val_pose_features = np.load(val_pose_features_output_path)\n",
        "test_pose_features = np.load(test_pose_features_output_path)\n",
        "\n",
        "# Print the shape and length of the loaded pose features\n",
        "print(f\"Train Pose features shape: {train_pose_features.shape}\")\n",
        "print(f\"Validation Pose features shape: {val_pose_features.shape}\")\n",
        "print(f\"Test Pose features shape: {test_pose_features.shape}\")\n",
        "\n",
        "# Print the number of samples for Pose features\n",
        "print(f\"Number of training samples (Pose features): {len(train_pose_features)}\")\n",
        "print(f\"Number of validation samples (Pose features): {len(val_pose_features)}\")\n",
        "print(f\"Number of test samples (Pose features): {len(test_pose_features)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_JVnEpgo5a0",
        "outputId": "53abf93b-2b15-485c-8e31-c0114030185c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set - Total: 9538, Detected pose landmarks: 9406, Zero vectors: 132\n",
            "Shape of pose features array: (9538, 92)\n",
            "Validation set - Total: 1017, Detected pose landmarks: 1016, Zero vectors: 1\n",
            "Shape of pose features array: (1017, 92)\n",
            "Test set - Total: 1123, Detected pose landmarks: 1123, Zero vectors: 0\n",
            "Shape of pose features array: (1123, 92)\n"
          ]
        }
      ],
      "source": [
        "# --- Verification and Summary ---\n",
        "def verify_pose_features(pose_features, dataset_name):\n",
        "    num_samples = len(pose_features)\n",
        "    num_detected = sum(1 for feature in pose_features if any(p != 0 for p in feature))\n",
        "    num_zero_vectors = num_samples - num_detected\n",
        "\n",
        "    print(f\"{dataset_name} - Total: {num_samples}, Detected pose landmarks: {num_detected}, Zero vectors: {num_zero_vectors}\")\n",
        "    print(f\"Shape of pose features array: {np.array(pose_features).shape}\")\n",
        "\n",
        "# Verify extracted features for each dataset\n",
        "verify_pose_features(train_pose_features, \"Train set\")\n",
        "verify_pose_features(val_pose_features, \"Validation set\")\n",
        "verify_pose_features(test_pose_features, \"Test set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYggU97Ro5ef",
        "outputId": "14f654a9-48fc-4137-befb-e2e738573d39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed Pose Detection Count per Class:\n",
            "Train set:\n",
            "  Class 'c0': 7\n",
            "  Class 'c1': 7\n",
            "  Class 'c2': 23\n",
            "  Class 'c4': 25\n",
            "  Class 'c6': 27\n",
            "  Class 'c7': 23\n",
            "  Class 'c8': 20\n",
            "Validation set:\n",
            "  Class 'c7': 1\n",
            "Test set:\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# --- Function to count failed detections per class ---\n",
        "def count_failed_detections(dataset, pose_features):\n",
        "    labels = [label for _, label in dataset.imgs]\n",
        "    failed_indices = [i for i, feature in enumerate(pose_features) if all(p == 0 for p in feature)]\n",
        "    failed_labels = [labels[i] for i in failed_indices]\n",
        "    return Counter(failed_labels)\n",
        "\n",
        "# Load datasets for labels\n",
        "train_dataset = datasets.ImageFolder(train_dir)\n",
        "val_dataset = datasets.ImageFolder(val_dir)\n",
        "test_dataset = datasets.ImageFolder(test_dir)\n",
        "\n",
        "# Count failures for each subset\n",
        "train_failures = count_failed_detections(train_dataset, train_pose_features)\n",
        "val_failures = count_failed_detections(val_dataset, val_pose_features)\n",
        "test_failures = count_failed_detections(test_dataset, test_pose_features)\n",
        "\n",
        "# Map class indices to class names\n",
        "class_names = train_dataset.classes  # Assuming same classes across all sets\n",
        "\n",
        "# Print results\n",
        "print(\"Failed Pose Detection Count per Class:\")\n",
        "\n",
        "print(\"Train set:\")\n",
        "for cls_idx, count in train_failures.items():\n",
        "    print(f\"  Class '{class_names[cls_idx]}': {count}\")\n",
        "\n",
        "print(\"Validation set:\")\n",
        "for cls_idx, count in val_failures.items():\n",
        "    print(f\"  Class '{class_names[cls_idx]}': {count}\")\n",
        "\n",
        "print(\"Test set:\")\n",
        "for cls_idx, count in test_failures.items():\n",
        "    print(f\"  Class '{class_names[cls_idx]}': {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5ew1HnHo5hl"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKgq1E_I2GA9",
        "outputId": "fadc8802-cdbe-4bb8-8a97-2f3e893dabd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set - Total: 9538, Valid features: 9406, Invalid (zero) features: 132\n",
            "Shape of pose features array: (9538, 92)\n",
            "Validation set - Total: 1017, Valid features: 1016, Invalid (zero) features: 1\n",
            "Shape of pose features array: (1017, 92)\n",
            "Test set - Total: 1123, Valid features: 1123, Invalid (zero) features: 0\n",
            "Shape of pose features array: (1123, 92)\n"
          ]
        }
      ],
      "source": [
        "def verify_all_features_present(features, dataset_name):\n",
        "    num_samples = len(features)\n",
        "    num_valid = sum(1 for feature in features if any(p != 0 for p in feature))\n",
        "    num_invalid = num_samples - num_valid\n",
        "\n",
        "    print(f\"{dataset_name} - Total: {num_samples}, Valid features: {num_valid}, Invalid (zero) features: {num_invalid}\")\n",
        "    print(f\"Shape of pose features array: {np.array(features).shape}\")\n",
        "\n",
        "# Verify for training set\n",
        "verify_all_features_present(train_pose_features, \"Train set\")\n",
        "\n",
        "# Verify for validation set\n",
        "verify_all_features_present(val_pose_features, \"Validation set\")\n",
        "\n",
        "# Verify for test set\n",
        "verify_all_features_present(test_pose_features, \"Test set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyWnPS6P24d_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def impute_missing_landmarks_directly(features, detected_flags, class_labels, global_mean=None):\n",
        "    for cls_idx in np.unique(class_labels):\n",
        "        # Get indices for the current class\n",
        "        class_indices = (class_labels == cls_idx)\n",
        "        detected = detected_flags[class_indices]\n",
        "\n",
        "        # Determine mean landmarks to use for imputation\n",
        "        if detected.any():\n",
        "            mean_landmarks = np.mean(features[class_indices][detected], axis=0)\n",
        "            print(f\"Class {cls_idx}: Using class mean for imputation.\")\n",
        "        else:\n",
        "            mean_landmarks = global_mean\n",
        "            print(f\"Class {cls_idx}: No valid samples, using global mean for imputation.\")\n",
        "\n",
        "        # Impute missing landmarks (zero vectors) directly in the features array\n",
        "        for i, index in enumerate(np.where(class_indices)[0]):\n",
        "            if not detected[i]:  # Only impute where detection flag is False\n",
        "                print(f\"Imputing missing data for index {index} in class {cls_idx}.\")\n",
        "                features[index] = mean_landmarks\n",
        "\n",
        "        # Confirm changes\n",
        "        first_zero_vector_idx = next((i for i, f in enumerate(features[class_indices]) if not np.any(f)), None)\n",
        "        if first_zero_vector_idx is not None:\n",
        "            print(f\"Class {cls_idx} still has zero vector at index {first_zero_vector_idx}.\")\n",
        "        else:\n",
        "            print(f\"Class {cls_idx} all vectors are valid now.\")\n",
        "\n",
        "    return features\n",
        "\n",
        "# Re-calculate global mean for fallback, excluding zero vectors\n",
        "global_mean_pose = np.mean(train_pose_features[train_detected_flags], axis=0)\n",
        "\n",
        "# Apply the imputation function directly\n",
        "train_pose_features = impute_missing_landmarks_directly(train_pose_features, train_detected_flags, train_class_labels, global_mean_pose)\n",
        "val_pose_features = impute_missing_landmarks_directly(val_pose_features, val_detected_flags, val_class_labels, global_mean_pose)\n",
        "test_pose_features = impute_missing_landmarks_directly(test_pose_features, test_detected_flags, test_class_labels, global_mean_pose)\n",
        "\n",
        "# Re-verify the features\n",
        "verify_all_features_present(train_pose_features, \"Train set\")\n",
        "verify_all_features_present(val_pose_features, \"Validation set\")\n",
        "verify_all_features_present(test_pose_features, \"Test set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyygOR86uCqV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Custom Callback for Plotting ---\n",
        "class TrainingPlot:\n",
        "    def __init__(self):\n",
        "        self.losses = []\n",
        "        self.acc = []\n",
        "        self.val_losses = []\n",
        "        self.val_acc = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        self.losses.append(logs['loss'])\n",
        "        self.acc.append(logs['accuracy'])\n",
        "        self.val_losses.append(logs['val_loss'])\n",
        "        self.val_acc.append(logs['val_accuracy'])\n",
        "\n",
        "        if len(self.losses) > 1:\n",
        "            plt.style.use(\"seaborn\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(self.losses, label=\"Training Loss\")\n",
        "            plt.plot(self.val_losses, label=\"Validation Loss\")\n",
        "            plt.title(\"Training and Validation Loss\")\n",
        "            plt.xlabel(\"Epoch #\")\n",
        "            plt.ylabel(\"Loss\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(self.acc, label=\"Training Accuracy\")\n",
        "            plt.plot(self.val_acc, label=\"Validation Accuracy\")\n",
        "            plt.title(\"Training and Validation Accuracy\")\n",
        "            plt.xlabel(\"Epoch #\")\n",
        "            plt.ylabel(\"Accuracy\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "plot_losses = TrainingPlot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l-jeJDctFhq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "num_epochs = 60\n",
        "# --- Custom Callback for Plotting ---\n",
        "class TrainingPlot(object):\n",
        "    def __init__(self, num_epochs):\n",
        "        self.num_epochs = num_epochs\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.accuracies = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.accuracies.append(logs.get('accuracy'))\n",
        "        self.val_accuracies.append(logs.get('val_accuracy'))\n",
        "\n",
        "        # Check if enough data points are available for plotting\n",
        "        if len(self.losses) > 1:\n",
        "\n",
        "            num_epochs = len(self.losses) # Use the actual number of epochs\n",
        "            plt.style.use(\"seaborn\")\n",
        "            plt.figure()\n",
        "            plt.plot(range(num_epochs), self.losses, label=\"Training Loss\")\n",
        "            plt.plot(range(num_epochs), self.val_losses, label=\"Validation Loss\")\n",
        "            plt.xlabel(\"Epoch\")\n",
        "            plt.ylabel(\"Loss\")\n",
        "            plt.legend()\n",
        "            plt.title(\"Training and Validation Loss\")\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(range(num_epochs), self.accuracies, label=\"Training Accuracy\")\n",
        "            plt.plot(range(num_epochs), self.val_accuracies, label=\"Validation Accuracy\")\n",
        "            plt.xlabel(\"Epoch\")\n",
        "            plt.ylabel(\"Accuracy\")\n",
        "            plt.legend()\n",
        "            plt.title(\"Training and Validation Accuracy\")\n",
        "            plt.show()\n",
        "\n",
        "plot_losses = TrainingPlot(num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1Ph13NSbzdm",
        "outputId": "57f418b1-4c3d-4be7-88e2-20746cc6555d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 9538\n",
            "Number of validation samples: 1017\n",
            "Number of test samples: 1123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch feature shape (train): torch.Size([32, 3, 224, 224])\n",
            "Batch label shape (train): torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# --- Constants and Mappings ---\n",
        "activity_map_AUC = {\n",
        "    'c0': 'Drive Safe',\n",
        "    'c1': 'Text Right',\n",
        "    'c2': 'Talk Right',\n",
        "    'c3': 'Text Left',\n",
        "    'c4': 'Talk Left',\n",
        "    'c5': 'Adjust Radio',\n",
        "    'c6': 'Drink',\n",
        "    'c7': 'Reach Behind',\n",
        "    'c8': 'Hair and Makeup',\n",
        "    'c9': 'Talk Passenger'\n",
        "}\n",
        "\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "BATCH_SIZE = 32 # Updated batch size as specified\n",
        "\n",
        "\n",
        "# --- Image Data Loading and Transformation ---\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Define dataset paths\n",
        "train_dir = '/content/drive/MyDrive/AUC/WV/train'\n",
        "val_dir = '/content/drive/MyDrive/AUC/WV/val'\n",
        "test_dir = '/content/drive/MyDrive/AUC/WV/test'\n",
        "\n",
        "# Load image datasets\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=val_transforms)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
        "\n",
        "# --- DataLoader Creation ---\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=2, pin_memory=True, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --- Dataset Verification ---\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")\n",
        "\n",
        "# Check a batch from the train_loader to verify data loading\n",
        "train_features, train_labels = next(iter(train_loader))\n",
        "print(f\"Batch feature shape (train): {train_features.size()}\")\n",
        "print(f\"Batch label shape (train): {train_labels.size()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dXmxe1wHguv"
      },
      "source": [
        "# **MODLE DD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj4CX-QPv37J",
        "outputId": "ab36d54d-c1ee-429d-de42-5a8baeed07a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import inception_v3\n",
        "\n",
        "# --- Model Definition for Driver Distraction Detection (As in the Paper) ---\n",
        "class InceptionV3BiLSTM(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(InceptionV3BiLSTM, self).__init__()\n",
        "        # Load pretrained InceptionV3 model\n",
        "        self.base_model = inception_v3(pretrained=True, aux_logits=True)\n",
        "        self.base_model.aux_logits = False\n",
        "        self.base_model.fc = nn.Identity()  # Remove final fully connected layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        # First BiLSTM layer to capture temporal features from spatial feature maps\n",
        "        self.bilstm1 = nn.LSTM(input_size=16384, hidden_size=14, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Second BiLSTM layer\n",
        "        self.bilstm2 = nn.LSTM(input_size=28, hidden_size=14, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.fc = nn.Linear(28, num_classes)  # 256 because BiLSTM is bidirectional with 128 hidden units\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from the base InceptionV3 model\n",
        "        for name, module in self.base_model.named_children():\n",
        "            if name == 'AuxLogits' and not self.base_model.aux_logits:\n",
        "                continue  # Skip the auxiliary logits if aux_logits is False\n",
        "            x = module(x)\n",
        "            if name == 'Mixed_7c':  # Stop at Mixed_7c layer\n",
        "                break\n",
        "\n",
        "        # Reshape: Flatten spatial dimensions (8x8) into the sequence dimension (8x8 = 64 timesteps)\n",
        "        x = x.view(x.size(0), -1, 16384)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through the first BiLSTM layer\n",
        "        x, _ = self.bilstm1(x)  # Output shape: (batch_size, 64, 256)\n",
        "\n",
        "        # Pass through the second BiLSTM layer\n",
        "        x, _ = self.bilstm2(x)  # Output shape: (batch_size, 64, 256)\n",
        "\n",
        "        # Use the last time step for classification\n",
        "        x = x[:, -1, :]  # (shape: batch_size, 256)\n",
        "\n",
        "        # Final classification layer\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate and move the model to the GPU\n",
        "model_dd = InceptionV3BiLSTM(num_classes=len(activity_map_AUC))\n",
        "model_dd = model_dd.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion_dd = nn.CrossEntropyLoss()\n",
        "optimizer_dd = optim.Adam(model_dd.parameters(), lr=0.0001)\n",
        "\n",
        "# Training and validation code remains the same as in your implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbVELMWhlPQD"
      },
      "outputs": [],
      "source": [
        "# --- Training and Validation Loop ---\n",
        "num_epochs = 60\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_patience = 10\n",
        "early_stopping_counter = 0\n",
        "best_model_wts_dd = copy.deepcopy(model_dd.state_dict())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_dd.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (data, target) in train_progress:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer_dd.zero_grad()\n",
        "        outputs = model_dd(data)\n",
        "        loss = criterion_dd(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer_dd.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        train_progress.set_postfix(loss=running_loss/(batch_idx+1), accuracy=correct/total)\n",
        "\n",
        "    # Validation phase\n",
        "    model_dd.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model_dd(data)\n",
        "            loss = criterion_dd(outputs, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    epoch_val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts_dd = copy.deepcopy(model_dd.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best model weights\n",
        "model_dd.load_state_dict(best_model_wts_dd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-bsLAGPEGUY"
      },
      "outputs": [],
      "source": [
        "# --- Evaluation and Reporting ---\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_dd.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model_dd(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for metric calculation\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate and print metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "class_names = list(activity_map_AUC.values())\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df_filtered = report_df.drop('accuracy', errors='ignore')\n",
        "print(tabulate(report_df_filtered, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names).plot(cmap='OrRd')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14_nfFZytYYt"
      },
      "source": [
        "### **MODEL DD+FER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggp92xqs596Y"
      },
      "outputs": [],
      "source": [
        "# Example batch input for Driver Distraction (DD) task\n",
        "inputs = torch.randn(8, 3, 299, 299).to(device)  # Random images for DD task\n",
        "\n",
        "# Example of extracted FER features\n",
        "fer_features = torch.randn(8, 512).to(device)  # Randomly generated FER features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7VaMB215Psi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "batch_size = 8\n",
        "fer_feature_size = 512\n",
        "num_classes = 10\n",
        "# --- Model Definition for DD + FER (Driver Distraction + Facial Emotion Recognition) Task ---\n",
        "class InceptionV3BiLSTM_FER(nn.Module):\n",
        "    def __init__(self, num_classes=10, fer_feature_size=512):\n",
        "        super(InceptionV3BiLSTM_FER, self).__init__()\n",
        "        self.base_model = inception_v3(weights='IMAGENET1K_V1', aux_logits=True)\n",
        "        self.base_model.aux_logits = False\n",
        "        self.base_model.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "        self.bilstm = nn.LSTM(input_size=2048, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # Additional layers for FER features\n",
        "        self.fc_fer = nn.Linear(fer_feature_size, 128)\n",
        "        self.fc_combined = nn.Linear(256 + 128, num_classes)  # Fully connected layer for classification\n",
        "\n",
        "    def forward(self, x, fer_features):\n",
        "        # Extract features from base model (DD task)\n",
        "        for name, module in self.base_model.named_children():\n",
        "            if name == 'AuxLogits' and not self.base_model.aux_logits:\n",
        "                continue  # Skip auxiliary logits if aux_logits is False\n",
        "            x = module(x)\n",
        "            if name == 'Mixed_7c':  # Extract feature map after Mixed_7c\n",
        "                break\n",
        "\n",
        "        x = x.view(x.size(0), -1, 2048)  # Reshape for BiLSTM (batch_size, 64, 2048)\n",
        "        x, _ = self.bilstm(x)  # Pass through BiLSTM\n",
        "        x = x[:, -1, :]  # Take output of the last time step\n",
        "\n",
        "        # Process FER features\n",
        "        fer_x = torch.relu(self.fc_fer(fer_features))\n",
        "\n",
        "        # Combine DD and FER features\n",
        "        combined_x = torch.cat((x, fer_x), dim=1)\n",
        "        combined_x = self.fc_combined(combined_x)\n",
        "        return combined_x\n",
        "\n",
        "# Instantiate and move the model to the GPU\n",
        "model_dd_fer = InceptionV3BiLSTM_FER(num_classes=len(activity_map_AUC), fer_feature_size=train_fer_features.shape[1])\n",
        "model_dd_fer = model_dd_fer.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion_dd_fer = nn.CrossEntropyLoss()\n",
        "optimizer_dd_fer = optim.Adam(model_dd_fer.parameters(), lr=0.00001, weight_decay=0.01)\n",
        "\n",
        "# Define inputs (DD and FER features)\n",
        "inputs = torch.randn(batch_size, 3, 299, 299).to(device)  # Example DD input\n",
        "fer_features = torch.randn(batch_size, fer_feature_size).to(device)  # Example FER features\n",
        "\n",
        "# Handle missing FER features\n",
        "if fer_features is None:\n",
        "    fer_features = torch.zeros(batch_size, fer_feature_size).to(device)\n",
        "\n",
        "# Forward pass with DD and FER features\n",
        "combined_output = model_dd_fer(inputs, fer_features)\n",
        "\n",
        "# Compute loss (example labels for classification task)\n",
        "labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
        "loss = criterion_dd_fer(combined_output, labels)\n",
        "\n",
        "# Backward pass and optimization\n",
        "loss.backward()\n",
        "optimizer_dd_fer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VD6khictkVk"
      },
      "outputs": [],
      "source": [
        "# --- Training and Validation Loop for DD + FER Model ---\n",
        "num_epochs = 60\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_patience = 10\n",
        "early_stopping_counter = 0\n",
        "best_model_wts_dd_fer = copy.deepcopy(model_dd_fer.state_dict())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_dd_fer.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (data, target) in train_progress:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer_dd_fer.zero_grad()\n",
        "\n",
        "        # Extract FER features for the current batch\n",
        "        batch_indices = list(range(batch_idx * train_loader.batch_size, min((batch_idx + 1) * train_loader.batch_size, len(train_fer_features))))\n",
        "        fer_batch_features = train_fer_features[batch_indices]\n",
        "\n",
        "        # Handle missing FER features by replacing with zero vectors\n",
        "        zero_vector = np.zeros(fer_batch_features.shape[1])\n",
        "        fer_batch_features = np.array([fer_batch_features[i] if train_face_detected[batch_indices[i]] == 1 else zero_vector for i in range(len(batch_indices))])\n",
        "\n",
        "        # Convert FER features to tensor and send to device\n",
        "        fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Debug: Print shapes\n",
        "        print(f\"Batch {batch_idx+1}: Data shape {data.shape}, FER features shape {fer_batch_features.shape}\")\n",
        "\n",
        "        # Check for mismatched batch sizes and skip if necessary\n",
        "        if len(fer_batch_features) != data.size(0):\n",
        "            print(f\"Skipping batch {batch_idx+1} due to mismatched FER features.\")\n",
        "            continue\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_dd_fer(data, fer_batch_features)\n",
        "        loss = criterion_dd_fer(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer_dd_fer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        train_progress.set_postfix(loss=running_loss/(batch_idx+1), accuracy=correct/total)\n",
        "\n",
        "\n",
        "    # Validation phase\n",
        "    model_dd_fer.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Extract FER features for the current batch\n",
        "            batch_indices = list(range(batch_idx * val_loader.batch_size, min((batch_idx + 1) * val_loader.batch_size, len(val_fer_features))))\n",
        "            fer_batch_features = val_fer_features[batch_indices]\n",
        "\n",
        "            # Handle missing FER features by replacing with zero vectors\n",
        "            fer_batch_features = np.array([fer_batch_features[i] if val_face_detected[batch_indices[i]] == 1 else zero_vector for i in range(len(batch_indices))])\n",
        "\n",
        "            # Convert FER features to tensor and send to device\n",
        "            fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "\n",
        "            # Ensure batch sizes match\n",
        "            if len(fer_batch_features) != data.size(0):\n",
        "                print(f\"Skipping validation batch {batch_idx+1} due to mismatched FER features.\")\n",
        "                continue\n",
        "\n",
        "            outputs = model_dd_fer(data, fer_batch_features)\n",
        "            loss = criterion_dd_fer(outputs, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    epoch_val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts_dd_fer = copy.deepcopy(model_dd_fer.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best model weights\n",
        "model_dd_fer.load_state_dict(best_model_wts_dd_fer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrJ85gtSWZLW"
      },
      "outputs": [],
      "source": [
        "# --- Evaluation and Reporting for DD + FER ---\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_dd_fer.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Disable gradient computation for inference\n",
        "with torch.no_grad():\n",
        "    for data, target in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # Get FER features for the batch\n",
        "        fer_batch_features = torch.tensor(test_fer_features[:len(data)]).to(device)\n",
        "        outputs = model_dd_fer(data, fer_batch_features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true.extend(target.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for metric calculation\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Generate classification report\n",
        "class_names = list(activity_map_AUC.values())\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df_filtered = report_df.drop('accuracy', errors='ignore')\n",
        "print(tabulate(report_df_filtered, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "display_c_m = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
        "plt.figure(figsize=(10, 9))\n",
        "display_c_m.plot(cmap='OrRd', xticks_rotation=25)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.title('Confusion Matrix - DD + FER', fontsize=24)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEeoE8pUsYwc"
      },
      "source": [
        "### **DD+FER.Attention Fusion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koycaeWzsYzr"
      },
      "outputs": [],
      "source": [
        "# --- Model Definition for DD + FER Attention ---\n",
        "class InceptionV3BiLSTM_FER_Attention(nn.Module):\n",
        "    def __init__(self, num_classes=10, fer_feature_size=512):\n",
        "        super(InceptionV3BiLSTM_FER_Attention, self).__init__()\n",
        "        self.base_model = inception_v3(weights='IMAGENET1K_V1', aux_logits=True)\n",
        "        self.base_model.aux_logits = False\n",
        "        self.base_model.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "        self.bilstm = nn.LSTM(input_size=2048, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.Linear(256, 1)  # Attention mechanism on CNN+LSTM features\n",
        "        self.fc_fer = nn.Linear(fer_feature_size, 128)\n",
        "        self.fc_combined = nn.Linear(256 + 128, 128)  # Fully connected layer for combined features\n",
        "        self.attention_combined = nn.Linear(128, 1)  # Attention mechanism after combining FER features\n",
        "        self.fc_final = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x, fer_features, fer_flags):\n",
        "        # Extract features from base model\n",
        "        for name, module in self.base_model.named_children():\n",
        "            if name == 'AuxLogits' and not self.base_model.aux_logits:\n",
        "                continue  # Skip the auxiliary logits if aux_logits is False\n",
        "            x = module(x)\n",
        "            if name == 'Mixed_7c':  # Extract feature map after Mixed_7c\n",
        "                break\n",
        "\n",
        "        x = x.view(x.size(0), -1, 2048)  # Reshape for BiLSTM (batch_size, sequence_length, input_size)\n",
        "        lstm_out, _ = self.bilstm(x)  # Pass through BiLSTM\n",
        "\n",
        "        # Apply attention on CNN+LSTM features\n",
        "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)  # Compute attention weights\n",
        "        attn_applied = torch.sum(attn_weights * lstm_out, dim=1)  # Apply attention weights\n",
        "\n",
        "        # Process FER features with flagging\n",
        "        fer_x = torch.zeros(attn_applied.size(0), 128, device=x.device)  # Initialize with zeros\n",
        "        valid_fer_indices = (fer_flags == 1).nonzero(as_tuple=True)[0]  # Get indices where FER features are valid\n",
        "        if valid_fer_indices.numel() > 0:\n",
        "            valid_fer_x = torch.relu(self.fc_fer(fer_features[valid_fer_indices]))\n",
        "            fer_x[valid_fer_indices] = valid_fer_x\n",
        "\n",
        "        # Combine DD and FER features\n",
        "        combined_x = torch.cat((attn_applied, fer_x), dim=1)\n",
        "        combined_x = torch.relu(self.fc_combined(combined_x))\n",
        "\n",
        "        # Apply attention after combining features\n",
        "        attn_combined_weights = torch.softmax(self.attention_combined(combined_x), dim=1)\n",
        "        attn_combined_applied = attn_combined_weights * combined_x\n",
        "\n",
        "        output = self.fc_final(attn_combined_applied)\n",
        "        return output\n",
        "\n",
        "# Instantiate and move the model to the GPU\n",
        "model_attention = InceptionV3BiLSTM_FER_Attention(num_classes=len(activity_map_AUC), fer_feature_size=train_fer_features.shape[1])\n",
        "model_attention = model_attention.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion_attention = nn.CrossEntropyLoss()\n",
        "optimizer_attention = optim.Adam(model_attention.parameters(), lr=0.0001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAzlCCKdsY2k"
      },
      "outputs": [],
      "source": [
        "# --- Training and Validation Loop for DD + FER Attention Model ---\n",
        "num_epochs = 60\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_patience = 10\n",
        "early_stopping_counter = 0\n",
        "best_model_wts_attention = copy.deepcopy(model_attention.state_dict())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_attention.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (data, target) in train_progress:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer_attention.zero_grad()\n",
        "\n",
        "        # Extract FER features for the current batch\n",
        "        batch_indices = list(range(batch_idx * train_loader.batch_size, min((batch_idx + 1) * train_loader.batch_size, len(train_fer_features))))\n",
        "        fer_batch_features = train_fer_features[batch_indices]\n",
        "        fer_flags = train_face_detected[batch_indices]\n",
        "\n",
        "        # Handle missing FER features by replacing with zero vectors\n",
        "        zero_vector = np.zeros(fer_batch_features.shape[1])\n",
        "        fer_batch_features = np.array([fer_batch_features[i] if train_face_detected[batch_indices[i]] == 1 else zero_vector for i in range(len(batch_indices))])\n",
        "\n",
        "        # Convert FER features and flags to tensors and send to device\n",
        "        fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "        fer_flags = torch.tensor(fer_flags, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Debug: Print shapes\n",
        "        print(f\"Batch {batch_idx+1}: Data shape {data.shape}, FER features shape {fer_batch_features.shape}\")\n",
        "\n",
        "        # Check for mismatched batch sizes and skip if necessary\n",
        "        if len(fer_batch_features) != data.size(0):\n",
        "            print(f\"Skipping batch {batch_idx+1} due to mismatched FER features.\")\n",
        "            continue\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_attention(data, fer_batch_features, fer_flags)\n",
        "        loss = criterion_attention(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer_attention.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        train_progress.set_postfix(loss=running_loss/(batch_idx+1), accuracy=correct/total)\n",
        "\n",
        "    # Validation phase\n",
        "    model_attention.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Extract FER features for the current batch\n",
        "            batch_indices = list(range(batch_idx * val_loader.batch_size, min((batch_idx + 1) * val_loader.batch_size, len(val_fer_features))))\n",
        "            fer_batch_features = val_fer_features[batch_indices]\n",
        "            fer_flags = val_face_detected[batch_indices]\n",
        "\n",
        "            # Handle missing FER features by replacing with zero vectors\n",
        "            fer_batch_features = np.array([fer_batch_features[i] if val_face_detected[batch_indices[i]] == 1 else zero_vector for i in range(len(batch_indices))])\n",
        "\n",
        "            # Convert FER features and flags to tensors and send to device\n",
        "            fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "            fer_flags = torch.tensor(fer_flags, dtype=torch.float32).to(device)\n",
        "\n",
        "            # Ensure batch sizes match\n",
        "            if len(fer_batch_features) != data.size(0):\n",
        "                print(f\"Skipping validation batch {batch_idx+1} due to mismatched FER features.\")\n",
        "                continue\n",
        "\n",
        "            outputs = model_attention(data, fer_batch_features, fer_flags)\n",
        "            loss = criterion_attention(outputs, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    epoch_val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts_attention = copy.deepcopy(model_attention.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best model weights\n",
        "model_attention.load_state_dict(best_model_wts_attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CNr4p7qB92n"
      },
      "outputs": [],
      "source": [
        "# --- Evaluation and Reporting for Attention Mechanism ---\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_attention.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Disable gradient computation for inference\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in tqdm(enumerate(test_loader), desc=\"Evaluating\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Get FER features and flags for the batch\n",
        "        fer_batch_features = torch.tensor(test_fer_features[batch_idx * test_loader.batch_size:(batch_idx + 1) * test_loader.batch_size]).to(device)\n",
        "        fer_flags = torch.tensor(test_face_detected[batch_idx * test_loader.batch_size:(batch_idx + 1) * test_loader.batch_size]).to(device)\n",
        "\n",
        "        # Ensure batch sizes match\n",
        "        if len(fer_batch_features) != data.size(0):\n",
        "            print(f\"Skipping test batch {batch_idx+1} due to mismatched FER features.\")\n",
        "            continue\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_attention(data, fer_batch_features, fer_flags)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true.extend(target.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for metric calculation\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Generate classification report\n",
        "class_names = list(activity_map_AUC.values())\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df_filtered = report_df.drop('accuracy', errors='ignore')\n",
        "print(tabulate(report_df_filtered, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "display_c_m = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
        "plt.figure(figsize=(10, 9))\n",
        "display_c_m.plot(cmap='OrRd', xticks_rotation=25)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.title('Confusion Matrix - DD + FER Attention', fontsize=24)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fdBguBliVjM"
      },
      "source": [
        "### **DD+FER.Self-attention Fusion**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aKUqpEFippF"
      },
      "outputs": [],
      "source": [
        "class InceptionV3BiLSTM_FER_SelfAttention(nn.Module):\n",
        "    def __init__(self, num_classes=10, fer_feature_size=512):\n",
        "        super(InceptionV3BiLSTM_FER_SelfAttention, self).__init__()\n",
        "        self.base_model = inception_v3(weights='IMAGENET1K_V1', aux_logits=True)\n",
        "        self.base_model.aux_logits = False\n",
        "        self.base_model.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "        self.bilstm = nn.LSTM(input_size=10240, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.self_attention = nn.MultiheadAttention(embed_dim=256, num_heads=8)  # Self-Attention mechanism\n",
        "        self.fc_fer = nn.Linear(fer_feature_size, 128)\n",
        "        self.fc_combined = nn.Linear(256 + 128, num_classes)  # Fully connected layer for classification\n",
        "\n",
        "    def forward(self, x, fer_features):\n",
        "        # Extract features from base model\n",
        "        for name, module in self.base_model.named_children():\n",
        "            if name == 'AuxLogits' and not self.base_model.aux_logits:\n",
        "                continue  # Skip the auxiliary logits if aux_logits is False\n",
        "            x = module(x)\n",
        "            if name == 'Mixed_7c':  # Extract feature map after Mixed_7c\n",
        "                break\n",
        "\n",
        "        x = x.view(x.size(0), -1, 10240)  # Reshape for BiLSTM (batch_size, sequence_length, input_size)\n",
        "        lstm_out, _ = self.bilstm(x)  # Pass through BiLSTM\n",
        "\n",
        "        # Apply self-attention\n",
        "        lstm_out = lstm_out.transpose(0, 1)  # Transpose for self-attention (sequence_length, batch_size, feature_dim)\n",
        "        self_attn_out, _ = self.self_attention(lstm_out, lstm_out, lstm_out)  # Self-attention\n",
        "\n",
        "        # Average pooling after self-attention\n",
        "        self_attn_out = self_attn_out.mean(dim=0)  # (batch_size, feature_dim)\n",
        "\n",
        "        # Process FER features\n",
        "        fer_x = torch.relu(self.fc_fer(fer_features))\n",
        "\n",
        "        # Combine DD and FER features\n",
        "        combined_x = torch.cat((self_attn_out, fer_x), dim=1)\n",
        "        combined_x = self.fc_combined(combined_x)\n",
        "        return combined_x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01JKaaTGDHbr"
      },
      "outputs": [],
      "source": [
        "def forward(self, x, fer_features):\n",
        "    # Extract features from base model\n",
        "    for name, module in self.base_model.named_children():\n",
        "        if name == 'AuxLogits' and not self.base_model.aux_logits:\n",
        "            continue  # Skip the auxiliary logits if aux_logits is False\n",
        "        x = module(x)\n",
        "        if name == 'Mixed_7c':  # Extract feature map after Mixed_7c\n",
        "            break\n",
        "\n",
        "    x = x.view(x.size(0), -1, 2048)  # Reshape for BiLSTM (batch_size, sequence_length, input_size)\n",
        "    lstm_out, _ = self.bilstm(x)  # Pass through BiLSTM\n",
        "\n",
        "    # Apply self-attention\n",
        "    self_attn_out, _ = self.self_attention(lstm_out, lstm_out, lstm_out)\n",
        "\n",
        "    # Handle missing FER features\n",
        "    if fer_features.size(0) != x.size(0):\n",
        "        # Create a zero tensor with the same size as fer_features except for the batch dimension\n",
        "        missing_fer_features = torch.zeros(x.size(0), fer_features.size(1)).to(x.device)\n",
        "        # Update the missing FER features for the current batch\n",
        "        missing_fer_features[:fer_features.size(0)] = fer_features\n",
        "        fer_features = missing_fer_features\n",
        "\n",
        "    fer_x = torch.relu(self.fc_fer(fer_features))\n",
        "\n",
        "    # Combine DD and FER features\n",
        "    combined_x = torch.cat((self_attn_out, fer_x), dim=1)\n",
        "    combined_x = self.fc_combined(combined_x)\n",
        "    return combined_x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwOtH6ebipsc"
      },
      "outputs": [],
      "source": [
        "# Instantiate and move the model to the GPU\n",
        "model_self_attention = InceptionV3BiLSTM_FER_SelfAttention(num_classes=len(activity_map_AUC), fer_feature_size=train_fer_features.shape[1])\n",
        "model_self_attention = model_self_attention.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion_self_attention = nn.CrossEntropyLoss()\n",
        "optimizer_self_attention = optim.Adam(model_self_attention.parameters(), lr=0.0001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n0Jv09Sipvt"
      },
      "outputs": [],
      "source": [
        "# --- Training and Validation Loop for DD + FER Self-Attention Model ---\n",
        "num_epochs = 60\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_patience = 10\n",
        "early_stopping_counter = 0\n",
        "best_model_wts_self_attention = copy.deepcopy(model_self_attention.state_dict())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_self_attention.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (data, target) in train_progress:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer_self_attention.zero_grad()\n",
        "\n",
        "        # Extract FER features for the current batch\n",
        "        batch_indices = list(range(batch_idx * train_loader.batch_size, min((batch_idx + 1) * train_loader.batch_size, len(train_fer_features))))\n",
        "        fer_batch_features = train_fer_features[batch_indices]\n",
        "\n",
        "        # Handle missing FER features by replacing with zero vectors\n",
        "        zero_vector = np.zeros(fer_batch_features.shape[1])\n",
        "        fer_batch_features = np.array([fer_batch_features[i] if train_face_detected[batch_indices[i]] == 1 else zero_vector for i in range(len(batch_indices))])\n",
        "\n",
        "        # Convert FER features to tensor and send to device\n",
        "        fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Debug: Print shapes\n",
        "        print(f\"Batch {batch_idx+1}: Data shape {data.shape}, FER features shape {fer_batch_features.shape}\")\n",
        "\n",
        "        # Check for mismatched batch sizes and skip if necessary\n",
        "        if len(fer_batch_features) != data.size(0):\n",
        "            print(f\"Skipping batch {batch_idx+1} due to mismatched FER features.\")\n",
        "            continue\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_self_attention(data, fer_batch_features)\n",
        "        loss = criterion_self_attention(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer_self_attention.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        train_progress.set_postfix(loss=running_loss/(batch_idx+1), accuracy=correct/total)\n",
        "\n",
        "    # Validation phase\n",
        "    model_self_attention.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Extract FER features for the current batch\n",
        "            batch_indices = list(range(batch_idx * val_loader.batch_size, min((batch_idx + 1) * val_loader.batch_size, len(val_fer_features))))\n",
        "            fer_batch_features = val_fer_features[batch_indices]\n",
        "\n",
        "            # Handle missing FER features by replacing with zero vectors\n",
        "            fer_batch_features = np.array([fer_batch_features[i] if val_face_detected[batch_indices[i]] == 1 else zero_vector for i in range(len(batch_indices))])\n",
        "\n",
        "            # Convert FER features to tensor and send to device\n",
        "            fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "\n",
        "            # Ensure batch sizes match\n",
        "            if len(fer_batch_features) != data.size(0):\n",
        "                print(f\"Skipping validation batch {batch_idx+1} due to mismatched FER features.\")\n",
        "                continue\n",
        "\n",
        "            outputs = model_self_attention(data, fer_batch_features)\n",
        "            loss = criterion_self_attention(outputs, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    epoch_val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts_self_attention = copy.deepcopy(model_self_attention.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best model weights\n",
        "model_self_attention.load_state_dict(best_model_wts_self_attention)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqd4_CJbip1-"
      },
      "outputs": [],
      "source": [
        "# --- Evaluation and Reporting for Self-Attention Mechanism ---\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_self_attention.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Disable gradient computation for inference\n",
        "with torch.no_grad():\n",
        "    for data, target in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # Get FER features for the batch\n",
        "        fer_batch_features = torch.tensor(test_fer_features[:len(data)]).to(device)\n",
        "        outputs = model_self_attention(data, fer_batch_features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true.extend(target.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for metric calculation\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Generate classification report\n",
        "class_names = list(activity_map_AUC.values())\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df_filtered = report_df.drop('accuracy', errors='ignore')\n",
        "print(tabulate(report_df_filtered, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "display_c_m = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
        "plt.figure(figsize=(10, 9))\n",
        "display_c_m.plot(cmap='OrRd', xticks_rotation=25)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.title('Confusion Matrix - Self-Attention Model', fontsize=24)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyv_VObRFRgy"
      },
      "source": [
        "### **DD+FER.SA2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djVza4boFZQz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import inception_v3\n",
        "\n",
        "class InceptionV3SelfAttentionFusion(nn.Module):\n",
        "    def __init__(self, num_classes=10, fer_feature_size=512):\n",
        "        super(InceptionV3SelfAttentionFusion, self).__init__()\n",
        "        self.base_model = inception_v3(weights='IMAGENET1K_V1', aux_logits=True)\n",
        "        self.base_model.aux_logits = False\n",
        "        self.base_model.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        # Linear layer to project FER features into the same dimension as the DD features\n",
        "        self.fc_fer = nn.Linear(fer_feature_size, 10240)\n",
        "\n",
        "        # Self-attention layer\n",
        "        self.self_attention = nn.MultiheadAttention(embed_dim=10240, num_heads=8)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.fc_combined = nn.Linear(10240, num_classes)\n",
        "\n",
        "    def forward(self, x, fer_features):\n",
        "        # Extract DD features from the base model\n",
        "        for name, module in self.base_model.named_children():\n",
        "            if name == 'AuxLogits' and not self.base_model.aux_logits:\n",
        "                continue  # Skip the auxiliary logits if aux_logits is False\n",
        "            x = module(x)\n",
        "            if name == 'Mixed_7c':  # Extract feature map after Mixed_7c\n",
        "                break\n",
        "\n",
        "        # Flatten the spatial dimensions of DD features\n",
        "        x = x.view(x.size(0), -1, 10240)  # Reshape for self-attention (batch_size, sequence_length, feature_dim)\n",
        "\n",
        "        # Project FER features into the same dimension as DD features and add them to the sequence\n",
        "        fer_features = self.fc_fer(fer_features).unsqueeze(1)  # (batch_size, 1, 10240)\n",
        "        combined_features = torch.cat((x, fer_features), dim=1)  # (batch_size, sequence_length + 1, 10240)\n",
        "\n",
        "        # Apply self-attention to the combined sequence\n",
        "        combined_features = combined_features.transpose(0, 1)  # Transpose for self-attention (sequence_length, batch_size, feature_dim)\n",
        "        attn_output, _ = self.self_attention(combined_features, combined_features, combined_features)\n",
        "        attn_output = attn_output.transpose(0, 1)  # (batch_size, sequence_length + 1, 10240)\n",
        "\n",
        "        # Pooling to aggregate the sequence into a single vector\n",
        "        pooled_output = attn_output.mean(dim=1)  # Mean pooling over the sequence\n",
        "\n",
        "        # Final classification\n",
        "        output = self.fc_combined(pooled_output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpFD-6JiFZUS"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the loss function (criterion)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer (you can choose other optimizers like AdamW or SGD if preferred)\n",
        "optimizer = optim.Adam(model_dd.parameters(), lr=0.0001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnGCNChWFZXh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "# --- Training and Validation Loop ---\n",
        "num_epochs = 60\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_patience = 10\n",
        "early_stopping_counter = 0\n",
        "best_model_wts_dd = copy.deepcopy(model_dd.state_dict())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_dd.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (data, target) in train_progress:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer_dd.zero_grad()\n",
        "        outputs = model_dd(data, fer_features)  # Assuming FER features are provided in your DataLoader\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer_dd.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        train_progress.set_postfix(loss=running_loss/(batch_idx+1), accuracy=correct/total)\n",
        "\n",
        "    # Validation phase\n",
        "    model_dd.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model_dd(data, fer_features)  # Assuming FER features are provided in your DataLoader\n",
        "            loss = criterion(outputs, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    epoch_val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Loss: {epoch_loss:.4f} \"\n",
        "          f\"Acc: {epoch_acc:.4f} \"\n",
        "          f\"Val Loss: {epoch_val_loss:.4f} \"\n",
        "          f\"Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping with restore best weights check\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts_dd = copy.deepcopy(model_dd.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= early_stopping_patience:\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# Load the best model weights\n",
        "model_dd.load_state_dict(best_model_wts_dd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTVJHfgjsXWU"
      },
      "source": [
        "### **DD+FER.A+SA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VEgLzezsXgn"
      },
      "outputs": [],
      "source": [
        "class InceptionV3BiLSTM_FER_BothAttention(nn.Module):\n",
        "    def __init__(self, num_classes=10, fer_feature_size=512):\n",
        "        super(InceptionV3BiLSTM_FER_BothAttention, self).__init__()\n",
        "        self.base_model = inception_v3(weights='IMAGENET1K_V1', aux_logits=True)\n",
        "        self.base_model.aux_logits = False\n",
        "        self.base_model.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "        self.bilstm = nn.LSTM(input_size=10240, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.Linear(256, 1)  # Attention mechanism on CNN+LSTM features\n",
        "        self.fc_fer = nn.Linear(fer_feature_size, 128)\n",
        "        self.fc_combined = nn.Linear(256 + 128, 256)  # Fully connected layer for combined features\n",
        "        self.self_attention = nn.MultiheadAttention(embed_dim=256, num_heads=8)  # Self-Attention mechanism after combining features\n",
        "        self.fc_final = nn.Linear(256, num_classes)  # Final classification layer\n",
        "\n",
        "    def forward(self, x, fer_features, fer_flags):\n",
        "        # Extract features from base model\n",
        "        for name, module in self.base_model.named_children():\n",
        "            if name == 'AuxLogits' and not self.base_model.aux_logits:\n",
        "                continue  # Skip the auxiliary logits if aux_logits is False\n",
        "            x = module(x)\n",
        "            if name == 'Mixed_7c':  # Extract feature map after Mixed_7c\n",
        "                break\n",
        "\n",
        "        x = x.view(x.size(0), -1, 10240)  # Reshape for BiLSTM (batch_size, sequence_length, input_size)\n",
        "        lstm_out, _ = self.bilstm(x)  # Pass through BiLSTM\n",
        "\n",
        "        # Apply attention on CNN+LSTM features\n",
        "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)  # Compute attention weights\n",
        "        attn_applied = torch.sum(attn_weights * lstm_out, dim=1)  # Apply attention weights\n",
        "\n",
        "        # Process FER features with flagging\n",
        "        fer_x = torch.zeros(attn_applied.size(0), 128, device=x.device)  # Initialize with zeros\n",
        "        valid_fer_indices = (fer_flags == 1).nonzero(as_tuple=True)[0]  # Get indices where FER features are valid\n",
        "        if valid_fer_indices.numel() > 0:\n",
        "            valid_fer_x = torch.relu(self.fc_fer(fer_features[valid_fer_indices]))\n",
        "            fer_x[valid_fer_indices] = valid_fer_x\n",
        "\n",
        "        # Combine DD and FER features\n",
        "        combined_x = torch.cat((attn_applied, fer_x), dim=1)\n",
        "        combined_x = torch.relu(self.fc_combined(combined_x))\n",
        "\n",
        "        # Apply self-attention after combining features\n",
        "        combined_x = combined_x.unsqueeze(0)  # Add sequence dimension for self-attention\n",
        "        self_attn_out, _ = self.self_attention(combined_x, combined_x, combined_x)\n",
        "        self_attn_out = self_attn_out.squeeze(0)  # Remove sequence dimension\n",
        "\n",
        "        output = self.fc_final(self_attn_out)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IfSZ5STsXk7"
      },
      "outputs": [],
      "source": [
        "# Instantiate and move the model to the GPU\n",
        "model_both_attention = InceptionV3BiLSTM_FER_BothAttention(num_classes=len(activity_map_AUC), fer_feature_size=train_fer_features.shape[1])\n",
        "model_both_attention = model_both_attention.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion_both_attention = nn.CrossEntropyLoss()\n",
        "optimizer_both_attention = optim.Adam(model_both_attention.parameters(), lr=0.0001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22PHwMZvsXoW"
      },
      "outputs": [],
      "source": [
        "# --- Training and Validation Loop for DD + FER Both Attention and Self-Attention Model ---\n",
        "num_epochs = 60\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_patience = 10\n",
        "early_stopping_counter = 0\n",
        "best_model_wts_both_attention = copy.deepcopy(model_both_attention.state_dict())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_both_attention.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (data, target) in train_progress:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer_both_attention.zero_grad()\n",
        "\n",
        "        # Extract FER features for the current batch\n",
        "        batch_indices = list(range(batch_idx * train_loader.batch_size, min((batch_idx + 1) * train_loader.batch_size, len(train_fer_features))))\n",
        "        fer_batch_features = train_fer_features[batch_indices]\n",
        "        fer_flags = train_face_detected[batch_indices]\n",
        "\n",
        "        # Handle missing FER features by replacing with zero vectors\n",
        "        zero_vector = np.zeros(fer_batch_features.shape[1])\n",
        "        fer_batch_features = np.array([fer_batch_features[i] if train_face_detected[batch_indices[i]] == 1 else zero_vector for i in range(len(batch_indices))])\n",
        "\n",
        "        # Convert FER features and flags to tensors and send to device\n",
        "        fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "        fer_flags = torch.tensor(fer_flags, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Debug: Print shapes\n",
        "        print(f\"Batch {batch_idx+1}: Data shape {data.shape}, FER features shape {fer_batch_features.shape}\")\n",
        "\n",
        "        # Check for mismatched batch sizes and skip if necessary\n",
        "        if len(fer_batch_features) != data.size(0):\n",
        "            print(f\"Skipping batch {batch_idx+1} due to mismatched FER features.\")\n",
        "            continue\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_both_attention(data, fer_batch_features, fer_flags)\n",
        "        loss = criterion_both_attention(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer_both_attention.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        train_progress.set_postfix(loss=running_loss/(batch_idx+1), accuracy=correct/total)\n",
        "\n",
        "    # Validation phase\n",
        "    model_both_attention.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Extract FER features for the current batch\n",
        "            batch_indices = list(range(batch_idx * val_loader.batch_size, min((batch_idx + 1) * val_loader.batch_size, len(val_fer_features))))\n",
        "            fer_batch_features = val_fer_features[batch_indices]\n",
        "            fer_flags = val_face_detected[batch_indices]\n",
        "\n",
        "            # Handle missing FER features by replacing with zero vectors\n",
        "            fer_batch_features = np.array([fer_batch_features[i] if val_face_detected[batch_indices[i]] == 1 else zero_vector for i in range(len(batch_indices))])\n",
        "\n",
        "            # Convert FER features and flags to tensors and send to device\n",
        "            fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "            fer_flags = torch.tensor(fer_flags, dtype=torch.float32).to(device)\n",
        "\n",
        "            # Ensure batch sizes match\n",
        "            if len(fer_batch_features) != data.size(0):\n",
        "                print(f\"Skipping validation batch {batch_idx+1} due to mismatched FER features.\")\n",
        "                continue\n",
        "\n",
        "            outputs = model_both_attention(data, fer_batch_features, fer_flags)\n",
        "            loss = criterion_both_attention(outputs, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    epoch_val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts_both_attention = copy.deepcopy(model_both_attention.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best model weights\n",
        "model_both_attention.load_state_dict(best_model_wts_both_attention)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l18PjQxS28p3"
      },
      "outputs": [],
      "source": [
        "# --- Evaluation and Reporting for Both Attention and Self-Attention Mechanism ---\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_both_attention.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Disable gradient computation for inference\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in tqdm(enumerate(test_loader), desc=\"Evaluating\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Get FER features and flags for the batch\n",
        "        fer_batch_features = torch.tensor(test_fer_features[batch_idx * test_loader.batch_size:(batch_idx + 1) * test_loader.batch_size]).to(device)\n",
        "        fer_flags = torch.tensor(test_face_detected[batch_idx * test_loader.batch_size:(batch_idx + 1) * test_loader.batch_size]).to(device)\n",
        "\n",
        "        # Ensure batch sizes match\n",
        "        if len(fer_batch_features) != data.size(0):\n",
        "            print(f\"Skipping test batch {batch_idx+1} due to mismatched FER features.\")\n",
        "            continue\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_both_attention(data, fer_batch_features, fer_flags)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true.extend(target.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for metric calculation\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Generate classification report\n",
        "class_names = list(activity_map_AUC.values())\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df_filtered = report_df.drop('accuracy', errors='ignore')\n",
        "print(tabulate(report_df_filtered, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "display_c_m = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
        "plt.figure(figsize=(10, 9))\n",
        "display_c_m.plot(cmap='OrRd', xticks_rotation=25)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.title('Confusion Matrix - DD + FER Both Attention and Self-Attention', fontsize=24)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SYtS96yP9ZM"
      },
      "source": [
        "### **DD+POSE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7SprlPMcte-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class InceptionV3BiLSTMWithPose(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(InceptionV3BiLSTMWithPose, self).__init__()\n",
        "        # Access Inception_V3_Weights using torchvision.models\n",
        "        self.base_model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1, aux_logits=True)\n",
        "        # Set aux_logits to False to make all layers trainable\n",
        "        self.base_model.aux_logits = False\n",
        "        # Remove the final fully connected layer\n",
        "        self.base_model.fc = nn.Identity()\n",
        "\n",
        "        # Define the BiLSTM with input size matching InceptionV3 output size\n",
        "        self.bilstm = nn.LSTM(input_size=2048, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Linear layer to process pose landmarks\n",
        "        self.pose_fc = nn.Linear(23 * 3, 128)  # 23 landmarks * 3 features each (x, y, z)\n",
        "\n",
        "        # Combine features from BiLSTM and pose\n",
        "        self.fc_combined = nn.Linear(256 + 128, num_classes)  # 256 from BiLSTM + 128 from pose landmarks\n",
        "\n",
        "    def forward(self, x, pose_landmarks):\n",
        "        # Extract features from base model\n",
        "        for name, module in self.base_model.named_children():\n",
        "            if name == 'AuxLogits' and not self.base_model.aux_logits:\n",
        "                continue  # Skip the auxiliary logits if aux_logits is False\n",
        "            x = module(x)\n",
        "            if name == 'Mixed_7c':  # Extract feature map after Mixed_7c\n",
        "                break\n",
        "\n",
        "        # Flatten the feature map to (batch_size, num_features)\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1, 2048)  # Reshape for LSTM (batch_size, seq_len=1, input_size=2048)\n",
        "\n",
        "        # BiLSTM expects input shape (batch_size, seq_len, input_size)\n",
        "        x, _ = self.bilstm(x)\n",
        "        x = x[:, -1, :]  # Use the last hidden state\n",
        "\n",
        "        pose_features = torch.relu(self.pose_fc(pose_landmarks))\n",
        "        combined_features = torch.cat((x, pose_features), dim=1)\n",
        "\n",
        "        output = self.fc_combined(combined_features)\n",
        "        return output\n",
        "\n",
        "# Instantiate and move the model to the device\n",
        "model = InceptionV3BiLSTMWithPose(num_classes=10)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyAYyWM_gD4y"
      },
      "outputs": [],
      "source": [
        "# --- Training and Validation Loop ---\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "early_stopping_patience = 5\n",
        "num_epochs = 60\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (data, target) in train_progress:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        pose_data = train_pose_features[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
        "        pose_data = torch.tensor(pose_data, dtype=torch.float32).to(device)  # Ensure pose_data is float32\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data, pose_data)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        train_progress.set_postfix(loss=running_loss/(batch_idx+1), accuracy=correct/total)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            pose_data = val_pose_features[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
        "            pose_data = torch.tensor(pose_data, dtype=torch.float32).to(device)  # Ensure pose_data is float32\n",
        "\n",
        "            outputs = model(data, pose_data)\n",
        "            loss = criterion(outputs, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    epoch_val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best model weights\n",
        "model.load_state_dict(best_model_wts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zupoc4ohHI59"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "def predict(model, test_loader, test_pose_features):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            # Extract pose landmarks for the current batch\n",
        "            pose_landmarks = test_pose_features[batch_idx * BATCH_SIZE:(batch_idx + 1) * BATCH_SIZE]\n",
        "            pose_landmarks = torch.tensor(pose_landmarks, dtype=torch.float32).to(device)\n",
        "\n",
        "            outputs = model(data, pose_landmarks)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(target.cpu().numpy())\n",
        "\n",
        "    return all_predictions, all_labels\n",
        "\n",
        "# Assuming 'test_pose_features' is available, similar to train and val pose features\n",
        "predictions, labels = predict(model, test_loader, test_pose_features)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(labels, predictions)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(labels, predictions)\n",
        "print(report)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(labels, predictions)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqzRO-kvN5F5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluation and Reporting\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        pose_data = test_pose_features[batch_idx * BATCH_SIZE:(batch_idx + 1) * BATCH_SIZE]\n",
        "        pose_data = torch.tensor(pose_data, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Print shapes for debugging\n",
        "        print(f\"Data shape: {data.shape}, Pose data shape: {pose_data.shape}\")\n",
        "\n",
        "        outputs = model_dd_pose(data, pose_data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true.extend(target.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # Print output shape\n",
        "        print(f\"Outputs shape: {outputs.shape}\")\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df_filtered = report_df.drop('accuracy', errors='ignore')\n",
        "print(tabulate(report_df_filtered, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names).plot(cmap='OrRd', xticks_rotation=25)\n",
        "plt.title('Confusion Matrix - DD + Pose', fontsize=24)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giTpCCEgkirc"
      },
      "source": [
        "### **DD+FER+Pose**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqS0Dbtlkiu5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class InceptionV3BiLSTM_FER_Pose(nn.Module):\n",
        "    def __init__(self, num_classes=10, fer_feature_size=512):\n",
        "        super(InceptionV3BiLSTM_FER_Pose, self).__init__()\n",
        "        self.base_model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1, aux_logits=True)\n",
        "        self.base_model.aux_logits = False\n",
        "        self.base_model.fc = nn.Identity()\n",
        "\n",
        "        # BiLSTM for temporal features\n",
        "        self.bilstm = nn.LSTM(input_size=2048, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Fully connected layers for FER and Pose features\n",
        "        self.fc_fer = nn.Linear(fer_feature_size, 128)\n",
        "        self.fc_pose = nn.Linear(23 * 4, 128)  # 23 landmarks * 3 features each (x, y, z)\n",
        "\n",
        "        # Final fully connected layer combining all features\n",
        "        self.fc_combined = nn.Linear(256 + 128 + 128, num_classes)  # 256 from BiLSTM, 128 from FER, 128 from Pose\n",
        "\n",
        "    def forward(self, x, fer_features, pose_landmarks):\n",
        "        # Extract features from the base model (InceptionV3)\n",
        "        for name, module in self.base_model.named_children():\n",
        "            if name == 'AuxLogits' and not self.base_model.aux_logits:\n",
        "                continue\n",
        "            x = module(x)\n",
        "            if name == 'Mixed_7c':  # Stop at Mixed_7c layer\n",
        "                break\n",
        "\n",
        "        # Flatten and pass through BiLSTM\n",
        "        x = x.view(x.size(0), -1, 2048)\n",
        "        x, _ = self.bilstm(x)\n",
        "        x = x[:, -1, :]  # Last time step\n",
        "\n",
        "        # Process FER features\n",
        "        fer_x = torch.relu(self.fc_fer(fer_features))\n",
        "\n",
        "        # Process Pose features\n",
        "        pose_x = torch.relu(self.fc_pose(pose_landmarks))\n",
        "\n",
        "        # Combine all features\n",
        "        combined_x = torch.cat((x, fer_x, pose_x), dim=1)\n",
        "        output = self.fc_combined(combined_x)\n",
        "        return output\n",
        "\n",
        "# Instantiate and move the model to the device\n",
        "model = InceptionV3BiLSTM_FER_Pose(num_classes=10, fer_feature_size=train_fer_features.shape[1])\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sixhrotkw89"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Training and Validation Loop ---\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "early_stopping_patience = 10\n",
        "num_epochs = 60\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (data, target) in train_progress:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Handle FER features\n",
        "        batch_indices = list(range(batch_idx * train_loader.batch_size, min((batch_idx + 1) * train_loader.batch_size, len(train_fer_features))))\n",
        "        fer_batch_features = train_fer_features[batch_indices]\n",
        "        zero_vector = np.zeros(fer_batch_features.shape[1])\n",
        "        fer_batch_features = np.array([\n",
        "            fer_batch_features[i] if train_face_detected[batch_indices[i]] == 1 else zero_vector\n",
        "            for i in range(len(batch_indices))\n",
        "        ])\n",
        "        fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Handle Pose features\n",
        "        pose_data = train_pose_features[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
        "        pose_data = torch.tensor(pose_data, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Ensure FER features and data batch sizes match\n",
        "        if len(fer_batch_features) != data.size(0):\n",
        "            print(f\"Skipping batch {batch_idx+1} due to mismatched FER features. Data shape: {data.shape}, FER shape: {fer_batch_features.shape}\")\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data, fer_batch_features, pose_data)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        train_progress.set_postfix(loss=running_loss/(batch_idx+1), accuracy=correct/total)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Handle FER features\n",
        "            batch_indices = list(range(batch_idx * val_loader.batch_size, min((batch_idx + 1) * val_loader.batch_size, len(val_fer_features))))\n",
        "            fer_batch_features = val_fer_features[batch_indices]\n",
        "            fer_batch_features = np.array([\n",
        "                fer_batch_features[i] if val_face_detected[batch_indices[i]] == 1 else zero_vector\n",
        "                for i in range(len(batch_indices))\n",
        "            ])\n",
        "            fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "\n",
        "            # Handle Pose features\n",
        "            pose_data = val_pose_features[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
        "            pose_data = torch.tensor(pose_data, dtype=torch.float32).to(device)\n",
        "\n",
        "            if len(fer_batch_features) != data.size(0):\n",
        "                print(f\"Skipping validation batch {batch_idx+1} due to mismatched FER features. Data shape: {data.shape}, FER shape: {fer_batch_features.shape}\")\n",
        "                continue\n",
        "\n",
        "            outputs = model(data, fer_batch_features, pose_data)\n",
        "            loss = criterion(outputs, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    epoch_val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SZCONJUkxBd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "\n",
        "# --- Evaluation for DD + FER + Pose ---\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        fer_batch_features = torch.tensor(test_fer_features[:len(data)], dtype=torch.float32).to(device)\n",
        "        pose_data = torch.tensor(test_pose_features[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE], dtype=torch.float32).to(device)\n",
        "\n",
        "        outputs = model(data, fer_batch_features, pose_data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true.extend(target.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df_filtered = report_df.drop('accuracy', errors='ignore')\n",
        "print(tabulate(report_df_filtered, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "display_c_m = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
        "plt.figure(figsize=(10, 9))\n",
        "display_c_m.plot(cmap='OrRd', xticks_rotation=25)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.title('Confusion Matrix - DD + FER + Pose', fontsize=24)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccgXhZddtbKv"
      },
      "source": [
        "### **DD+FER+POSE(SA)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lG7OTnktbPS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class InceptionV3BiLSTM_FER_Pose_SelfAttention(nn.Module):\n",
        "    def __init__(self, num_classes=10, fer_feature_size=512, num_heads=8, attn_drop=0.1):\n",
        "        super(InceptionV3BiLSTM_FER_Pose_SelfAttention, self).__init__()\n",
        "        self.base_model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1, aux_logits=True)\n",
        "        self.base_model.aux_logits = False\n",
        "        self.base_model.fc = nn.Identity()\n",
        "\n",
        "        # BiLSTM for temporal features\n",
        "        self.bilstm = nn.LSTM(input_size=2048, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Fully connected layers for FER and Pose features\n",
        "        self.fc_fer = nn.Linear(fer_feature_size, 128)\n",
        "        self.fc_pose = nn.Linear(23 * 4, 128)\n",
        "\n",
        "        # Self-attention layer for fusion\n",
        "        self.self_attention = nn.MultiheadAttention(embed_dim=256 + 128 + 128, num_heads=num_heads, dropout=attn_drop)\n",
        "\n",
        "        # Final fully connected layer\n",
        "        self.fc_combined = nn.Linear(256 + 128 + 128, num_classes)\n",
        "\n",
        "    def forward(self, x, fer_features, pose_landmarks):\n",
        "        for name, module in self.base_model.named_children():\n",
        "            if name == 'AuxLogits' and not self.base_model.aux_logits:\n",
        "                continue\n",
        "            x = module(x)\n",
        "            if name == 'Mixed_7c':\n",
        "                break\n",
        "\n",
        "        # BiLSTM for temporal features\n",
        "        x = x.view(x.size(0), -1, 2048)\n",
        "        x, _ = self.bilstm(x)\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # Process FER and Pose features\n",
        "        fer_x = torch.relu(self.fc_fer(fer_features))\n",
        "        pose_x = torch.relu(self.fc_pose(pose_landmarks))\n",
        "\n",
        "        # Concatenate all features and apply self-attention\n",
        "        combined_x = torch.cat((x, fer_x, pose_x), dim=1).unsqueeze(1)\n",
        "        combined_x = combined_x.permute(1, 0, 2)  # (seq_len, batch, embedding)\n",
        "        attn_output, _ = self.self_attention(combined_x, combined_x, combined_x)\n",
        "        attn_output = attn_output.squeeze(0)  # Remove sequence length dimension\n",
        "\n",
        "        output = self.fc_combined(attn_output)\n",
        "        return output\n",
        "\n",
        "# Instantiate and move the model to the device\n",
        "model = InceptionV3BiLSTM_FER_Pose_SelfAttention(num_classes=10, fer_feature_size=train_fer_features.shape[1])\n",
        "model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3STyQh5lt2gB"
      },
      "outputs": [],
      "source": [
        "# --- Training and Validation Loop ---\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Early stopping parameters\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "early_stopping_patience = 5\n",
        "num_epochs = 60\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (data, target) in train_progress:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Handle FER features\n",
        "        batch_indices = list(range(batch_idx * train_loader.batch_size, min((batch_idx + 1) * train_loader.batch_size, len(train_fer_features))))\n",
        "        fer_batch_features = train_fer_features[batch_indices]\n",
        "        zero_vector = np.zeros(fer_batch_features.shape[1])\n",
        "        fer_batch_features = np.array([\n",
        "            fer_batch_features[i] if train_face_detected[batch_indices[i]] == 1 else zero_vector\n",
        "            for i in range(len(batch_indices))\n",
        "        ])\n",
        "        fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Handle Pose features\n",
        "        pose_data = train_pose_features[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
        "        pose_data = torch.tensor(pose_data, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Ensure FER features and data batch sizes match\n",
        "        if len(fer_batch_features) != data.size(0):\n",
        "            print(f\"Skipping batch {batch_idx+1} due to mismatched FER features. Data shape: {data.shape}, FER shape: {fer_batch_features.shape}\")\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data, fer_batch_features, pose_data)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        train_progress.set_postfix(loss=running_loss/(batch_idx+1), accuracy=correct/total)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Handle FER features\n",
        "            batch_indices = list(range(batch_idx * val_loader.batch_size, min((batch_idx + 1) * val_loader.batch_size, len(val_fer_features))))\n",
        "            fer_batch_features = val_fer_features[batch_indices]\n",
        "            fer_batch_features = np.array([\n",
        "                fer_batch_features[i] if val_face_detected[batch_indices[i]] == 1 else zero_vector\n",
        "                for i in range(len(batch_indices))\n",
        "            ])\n",
        "            fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "\n",
        "            # Handle Pose features\n",
        "            pose_data = val_pose_features[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
        "            pose_data = torch.tensor(pose_data, dtype=torch.float32).to(device)\n",
        "\n",
        "            if len(fer_batch_features) != data.size(0):\n",
        "                print(f\"Skipping validation batch {batch_idx+1} due to mismatched FER features. Data shape: {data.shape}, FER shape: {fer_batch_features.shape}\")\n",
        "                continue\n",
        "\n",
        "            outputs = model(data, fer_batch_features, pose_data)\n",
        "            loss = criterion(outputs, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    epoch_val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best model weights\n",
        "model.load_state_dict(best_model_wts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oxiqswucjQt"
      },
      "outputs": [],
      "source": [
        "# --- Evaluation and Reporting ---\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "# Disable gradient computation for inference\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in tqdm(enumerate(test_loader), desc=\"Evaluating\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Extract FER features for the batch\n",
        "        batch_size = data.size(0)\n",
        "        batch_indices = list(range(batch_idx * batch_size, min((batch_idx + 1) * batch_size, len(test_fer_features))))\n",
        "        fer_batch_features = test_fer_features[batch_indices]\n",
        "        zero_vector = np.zeros(fer_batch_features.shape[1])\n",
        "        fer_batch_features = np.array([\n",
        "            fer_batch_features[i] if test_face_detected[batch_indices[i]] == 1 else zero_vector\n",
        "            for i in range(len(batch_indices))\n",
        "        ])\n",
        "        fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Handle Pose features\n",
        "        pose_data = test_pose_features[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
        "        pose_data = torch.tensor(pose_data, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Ensure all feature sizes match the batch size\n",
        "        if len(fer_batch_features) != batch_size or len(pose_data) != batch_size:\n",
        "            print(f\"Skipping batch {batch_idx+1} due to mismatched feature sizes.\")\n",
        "            continue\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(data, fer_batch_features, pose_data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true.extend(target.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for metric calculation\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Generate classification report\n",
        "class_names = list(activity_map_AUC.values())\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df_filtered = report_df.drop('accuracy', errors='ignore')\n",
        "print(tabulate(report_df_filtered, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "display_c_m = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
        "plt.figure(figsize=(10, 9))\n",
        "display_c_m.plot(cmap='OrRd', xticks_rotation=25)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.title('Confusion Matrix', fontsize=24)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq0DOK_UpPk-"
      },
      "source": [
        "### **DD+POSE.A**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ8IlsHPpZdg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class InceptionV3BiLSTMWithPoseAttention(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(InceptionV3BiLSTMWithPoseAttention, self).__init__()\n",
        "        # Access Inception_V3_Weights using torchvision.models\n",
        "        self.base_model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1, aux_logits=True)\n",
        "        self.base_model.aux_logits = False\n",
        "        self.base_model.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        # Define the BiLSTM with input size matching InceptionV3 output size\n",
        "        self.bilstm = nn.LSTM(input_size=2048, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Linear layer to process pose landmarks\n",
        "        self.pose_fc = nn.Linear(23 * 3, 128)  # 23 landmarks * 3 features each (x, y, z)\n",
        "\n",
        "        # Linear layers to transform both feature sets to a common space for attention\n",
        "        self.bi_lstm_fc = nn.Linear(256, 128)  # Project BiLSTM features to 128 dimensions\n",
        "        self.pose_fc_transformed = nn.Linear(128, 128)  # Project pose features to 128 dimensions\n",
        "\n",
        "        # Attention mechanism to fuse BiLSTM and pose features\n",
        "        self.attention = nn.Linear(128, 1)  # Attention mechanism for intermediate fusion\n",
        "\n",
        "        # Final classification layer\n",
        "        self.fc_final = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x, pose_landmarks):\n",
        "        # Extract features from base model\n",
        "        for name, module in self.base_model.named_children():\n",
        "            if name == 'AuxLogits' and not self.base_model.aux_logits:\n",
        "                continue  # Skip the auxiliary logits if aux_logits is False\n",
        "            x = module(x)\n",
        "            if name == 'Mixed_7c':  # Extract feature map after Mixed_7c\n",
        "                break\n",
        "\n",
        "        # Flatten the feature map to (batch_size, num_features)\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1, 2048)  # Reshape for LSTM (batch_size, seq_len=1, input_size=2048)\n",
        "\n",
        "        # BiLSTM expects input shape (batch_size, seq_len, input_size)\n",
        "        x, _ = self.bilstm(x)\n",
        "        x = x[:, -1, :]  # Use the last hidden state\n",
        "        x = torch.relu(self.bi_lstm_fc(x))  # Project to a common feature space\n",
        "\n",
        "        # Process pose landmarks and project to the same space\n",
        "        pose_features = torch.relu(self.pose_fc(pose_landmarks))\n",
        "        pose_features = torch.relu(self.pose_fc_transformed(pose_features))\n",
        "\n",
        "        # Apply attention mechanism for intermediate fusion\n",
        "        combined_features = torch.stack((x, pose_features), dim=1)  # Stack along a new dimension\n",
        "        attn_weights = torch.softmax(self.attention(combined_features), dim=1)  # Compute attention weights\n",
        "        attn_applied = torch.sum(attn_weights * combined_features, dim=1)  # Apply attention weights\n",
        "\n",
        "        # Final output\n",
        "        output = self.fc_final(attn_applied)\n",
        "        return output\n",
        "\n",
        "# Instantiate and move the model to the device\n",
        "model_attention_fusion = InceptionV3BiLSTMWithPoseAttention(num_classes=10)\n",
        "model_attention_fusion.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion_attention_fusion = nn.CrossEntropyLoss()\n",
        "optimizer_attention_fusion = optim.Adam(model_attention_fusion.parameters(), lr=0.0001)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLLC-FFHrFE-"
      },
      "outputs": [],
      "source": [
        "# --- Training and Validation Loop ---\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "# --- Training and Validation Loop ---\n",
        "best_model_wts_attention_fusion = copy.deepcopy(model_attention_fusion.state_dict())\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "early_stopping_patience = 5\n",
        "num_epochs = 60\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_attention_fusion.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (data, target) in train_progress:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        pose_data = train_pose_features[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
        "        pose_data = torch.tensor(pose_data, dtype=torch.float32).to(device)  # Ensure pose_data is float32\n",
        "\n",
        "        optimizer_attention_fusion.zero_grad()\n",
        "        outputs = model_attention_fusion(data, pose_data)\n",
        "        loss = criterion_attention_fusion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer_attention_fusion.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        train_progress.set_postfix(loss=running_loss/(batch_idx+1), accuracy=correct/total)\n",
        "\n",
        "    # Validation phase\n",
        "    model_attention_fusion.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            pose_data = val_pose_features[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
        "            pose_data = torch.tensor(pose_data, dtype=torch.float32).to(device)  # Ensure pose_data is float32\n",
        "\n",
        "            outputs = model_attention_fusion(data, pose_data)\n",
        "            loss = criterion_attention_fusion(outputs, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    epoch_val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts_attention_fusion = copy.deepcopy(model_attention_fusion.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best model weights\n",
        "model_attention_fusion.load_state_dict(best_model_wts_attention_fusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV2HVu32rFCf"
      },
      "outputs": [],
      "source": [
        "# --- Evaluation and Reporting ---\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_attention_fusion.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "# Disable gradient computation for inference\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in tqdm(enumerate(test_loader), desc=\"Evaluating\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Extract FER features for the batch\n",
        "        batch_size = data.size(0)\n",
        "        batch_indices = list(range(batch_idx * batch_size, min((batch_idx + 1) * batch_size, len(test_fer_features))))\n",
        "        fer_batch_features = test_fer_features[batch_indices]\n",
        "        zero_vector = np.zeros(fer_batch_features.shape[1])\n",
        "        fer_batch_features = np.array([\n",
        "            fer_batch_features[i] if test_face_detected[batch_indices[i]] == 1 else zero_vector\n",
        "            for i in range(len(batch_indices))\n",
        "        ])\n",
        "        fer_batch_features = torch.tensor(fer_batch_features, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Handle Pose features\n",
        "        pose_data = test_pose_features[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
        "        pose_data = torch.tensor(pose_data, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Ensure all feature sizes match the batch size\n",
        "        if len(fer_batch_features) != batch_size or len(pose_data) != batch_size:\n",
        "            print(f\"Skipping batch {batch_idx+1} due to mismatched feature sizes.\")\n",
        "            continue\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_attention_fusion(data, pose_data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true.extend(target.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for metric calculation\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Generate classification report\n",
        "class_names = list(activity_map_AUC.values())\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df_filtered = report_df.drop('accuracy', errors='ignore')\n",
        "print(tabulate(report_df_filtered, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "display_c_m = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
        "plt.figure(figsize=(10, 9))\n",
        "display_c_m.plot(cmap='OrRd', xticks_rotation=25)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.title('Confusion Matrix - DD + FER + Pose Attention Fusion', fontsize=24)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bTIcp4bH7XR"
      },
      "source": [
        "# DD.SSoftmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65rc_PrmICng"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import inception_v3\n",
        "\n",
        "# --- Model Definition for DD (Driver Distraction) Task ---\n",
        "class InceptionV3BiLSTM(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(InceptionV3BiLSTM, self).__init__()\n",
        "        # Load pretrained weights with aux_logits=True for compatibility\n",
        "        self.base_model = inception_v3(pretrained=True, aux_logits=True)\n",
        "\n",
        "        # Set aux_logits to False to make all layers trainable\n",
        "        self.base_model.aux_logits = False\n",
        "\n",
        "        # Remove the final fully connected layer\n",
        "        self.base_model.fc = nn.Identity()\n",
        "\n",
        "        # Bidirectional LSTM\n",
        "        self.bilstm = nn.LSTM(input_size=10240, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(256, num_classes)  # Fully connected layer for classification\n",
        "        self.scoresoftmax = ScoreSoftmax(num_classes)  # Custom ScoreSoftmax layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from base model\n",
        "        for name, module in self.base_model.named_children():\n",
        "            if name == 'AuxLogits' and not self.base_model.aux_logits:\n",
        "                continue  # Skip the auxiliary logits if aux_logits is False\n",
        "            x = module(x)\n",
        "            if name == 'Mixed_7c':  # Extract feature map after Mixed_7c\n",
        "                break\n",
        "\n",
        "        # Flatten the feature map and prepare for BiLSTM\n",
        "        x = x.view(x.size(0), 5, 10240)\n",
        "        x, _ = self.bilstm(x)  # Pass through BiLSTM\n",
        "        x = self.fc(x[:, -1, :])  # Pass through final fully connected layer\n",
        "        x = self.scoresoftmax(x)  # Apply ScoreSoftmax\n",
        "        return x\n",
        "\n",
        "# Define the ScoreSoftmax layer\n",
        "class ScoreSoftmax(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ScoreSoftmax, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.score_layer = nn.Linear(num_classes, num_classes)  # Custom scoring layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = self.score_layer(x)  # Calculate scores for each class\n",
        "        return nn.functional.softmax(scores, dim=-1)  # Apply softmax to the scores\n",
        "\n",
        "# Instantiate and move the model to the GPU\n",
        "model_dd = InceptionV3BiLSTM(num_classes=len(activity_map_AUC))\n",
        "model_dd = model_dd.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion_dd = nn.CrossEntropyLoss()\n",
        "optimizer_dd = optim.Adam(model_dd.parameters(), lr=0.0001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n-qsq3oIq1n"
      },
      "outputs": [],
      "source": [
        "# --- Training and Validation Loop ---\n",
        "best_model_wts = copy.deepcopy(model_dd.state_dict())\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "early_stopping_patience = 5\n",
        "num_epochs = 60\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_dd.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer_dd.zero_grad()\n",
        "        outputs = model_dd(data)\n",
        "        loss = criterion_dd(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer_dd.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_acc)\n",
        "\n",
        "    # Validation phase\n",
        "    model_dd.eval()\n",
        "    val_running_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(val_loader, desc=\"Validation\"):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            outputs = model_dd(data)\n",
        "            val_loss = criterion_dd(outputs, target)\n",
        "            val_running_loss += val_loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_val_loss = val_running_loss / len(val_loader)\n",
        "    epoch_val_acc = val_correct / val_total\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    val_accuracies.append(epoch_val_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping and saving the best model\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts = copy.deepcopy(model_dd.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best model weights\n",
        "model_dd.load_state_dict(best_model_wts)\n",
        "\n",
        "# --- Evaluation on Test Data ---\n",
        "# (Evaluation code as provided previously)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQZtrG2eIsR_"
      },
      "outputs": [],
      "source": [
        "# --- Evaluation and Reporting ---\n",
        "# Set the model to evaluation mode\n",
        "model_dd.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "# Disable gradient computation for inference\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in tqdm(enumerate(test_loader), desc=\"Evaluating\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_dd(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true.extend(target.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for metric calculation\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Generate classification report\n",
        "class_names = list(activity_map_AUC.values())\n",
        "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df_filtered = report_df.drop('accuracy', errors='ignore')\n",
        "print(tabulate(report_df_filtered, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "display_c_m = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
        "plt.figure(figsize=(10, 9))\n",
        "display_c_m.plot(cmap='OrRd', xticks_rotation=25)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.title('Confusion Matrix', fontsize=24)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWHqn98yc7Vc"
      },
      "source": [
        "### **ViT-DD-FER-Pose**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIhi894Zc6kU"
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evdWQnynVM5-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models import create_model\n",
        "from timm.models.vision_transformer import VisionTransformer\n",
        "\n",
        "class ViTDD_FER_Pose(nn.Module):\n",
        "    def __init__(self, model_name='deit3_small_patch16_224', num_classes=10, fer_feature_size=512, pose_feature_size=69, drop_rate=0.05, drop_path_rate=0.11, img_size=224):\n",
        "        super(ViTDD_FER_Pose, self).__init__()\n",
        "        # Initialize the Vision Transformer model\n",
        "        self.backbone: VisionTransformer = create_model(\n",
        "            model_name,\n",
        "            pretrained=True,\n",
        "            num_classes=num_classes,  # The final classification head will be redefined\n",
        "            drop_rate=drop_rate,\n",
        "            drop_path_rate=drop_path_rate,\n",
        "            img_size=img_size\n",
        "        )\n",
        "        # Remove the original classifier head as it will be replaced\n",
        "        self.backbone.head = nn.Identity()\n",
        "\n",
        "        self.embed_dim = self.backbone.embed_dim  # Dimension of the transformer embeddings\n",
        "\n",
        "        # Linear layers for additional features\n",
        "        self.fc_fer = nn.Linear(fer_feature_size, 128)\n",
        "        self.fc_pose = nn.Linear(pose_feature_size, 128)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.fc_combined = nn.Linear(self.embed_dim + 128 + 128, num_classes)  # Combine all features\n",
        "\n",
        "    def forward(self, x, fer_features, pose_features):\n",
        "        # Extract features from the Vision Transformer (ViT) model\n",
        "        x = self.backbone(x)  # Extract embeddings\n",
        "\n",
        "        # Check if x has an expected shape\n",
        "        if x.ndim == 3:  # The correct output from transformer should have shape (batch_size, num_tokens, embed_dim)\n",
        "            cls_token = x[:, 0, :]  # Shape: (batch_size, embed_dim)\n",
        "        elif x.ndim == 2:  # Case where the output is already flattened\n",
        "            cls_token = x\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected tensor shape for x: {x.shape}\")\n",
        "\n",
        "        # Process FER features\n",
        "        fer_features = torch.relu(self.fc_fer(fer_features))  # Shape: (batch_size, 128)\n",
        "\n",
        "        # Process Pose features\n",
        "        pose_features = torch.relu(self.fc_pose(pose_features))  # Shape: (batch_size, 128)\n",
        "\n",
        "        # Debugging print statements\n",
        "        print(f\"Shape of class token (cls_token): {cls_token.shape}\")\n",
        "        print(f\"Shape of FER features (fer_features): {fer_features.shape}\")\n",
        "        print(f\"Shape of Pose features (pose_features): {pose_features.shape}\")\n",
        "\n",
        "        # Concatenate all features along the last dimension\n",
        "        combined_features = torch.cat([cls_token, fer_features, pose_features], dim=1)  # Shape: (batch_size, combined_dim)\n",
        "\n",
        "        # Debugging print statement to check shape after concatenation\n",
        "        print(f\"Shape of combined features: {combined_features.shape}\")\n",
        "\n",
        "        # Final classification\n",
        "        output = self.fc_combined(combined_features)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Instantiate and move the model to the device\n",
        "num_classes = len(activity_map_AUC)\n",
        "fer_feature_size = train_fer_features.shape[1]\n",
        "pose_feature_size = train_pose_features.shape[1]\n",
        "model = ViTDD_FER_Pose(num_classes=num_classes, fer_feature_size=fer_feature_size, pose_feature_size=pose_feature_size)\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzRGP9C3xlww"
      },
      "outputs": [],
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-3vAbJRDVM2u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "# Assuming ViTDD_FER_Pose model and other necessary setups are defined\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "num_classes = len(activity_map_AUC)\n",
        "fer_feature_size = train_fer_features.shape[1]\n",
        "pose_feature_size = train_pose_features.shape[1]\n",
        "model = ViTDD_FER_Pose(num_classes=num_classes, fer_feature_size=fer_feature_size, pose_feature_size=pose_feature_size)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# --- Training and Validation Loop ---\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "early_stopping_patience = 15\n",
        "num_epochs = 60\n",
        "\n",
        "# Zero vectors for cases where FER or Pose features are missing\n",
        "zero_vector_fer = torch.zeros((fer_feature_size,), dtype=torch.float32, device=device)\n",
        "zero_vector_pose = torch.zeros((pose_feature_size,), dtype=torch.float32, device=device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (data, target) in train_progress:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Handle FER features\n",
        "        batch_indices = list(range(batch_idx * train_loader.batch_size, min((batch_idx + 1) * train_loader.batch_size, len(train_fer_features))))\n",
        "        if len(batch_indices) == 0:\n",
        "            continue\n",
        "        fer_batch_features = torch.tensor(train_fer_features[batch_indices], dtype=torch.float32, device=device)\n",
        "        fer_batch_features = torch.stack([\n",
        "            fer_batch_features[i] if train_face_detected[batch_indices[i]] == 1 else zero_vector_fer\n",
        "            for i in range(len(batch_indices))\n",
        "        ])\n",
        "\n",
        "        # Handle Pose features\n",
        "        pose_batch_indices = list(range(batch_idx * train_loader.batch_size, min((batch_idx + 1) * train_loader.batch_size, len(train_pose_features))))\n",
        "        if len(pose_batch_indices) == 0:\n",
        "            continue\n",
        "        pose_data = torch.tensor([\n",
        "            train_pose_features[i] if i < len(train_pose_features) else zero_vector_pose\n",
        "            for i in pose_batch_indices\n",
        "        ], dtype=torch.float32).to(device)\n",
        "\n",
        "        # Check for mismatched batch sizes\n",
        "        if len(fer_batch_features) != data.size(0) or len(pose_data) != data.size(0):\n",
        "           # print(f\"Skipping batch {batch_idx+1} due to mismatched features.\")\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data, fer_batch_features, pose_data)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        train_progress.set_postfix(loss=running_loss / (batch_idx + 1), accuracy=correct / total)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Handle FER features\n",
        "            batch_indices = list(range(batch_idx * val_loader.batch_size, min((batch_idx + 1) * val_loader.batch_size, len(val_fer_features))))\n",
        "            if len(batch_indices) == 0:\n",
        "                continue\n",
        "            fer_batch_features = torch.tensor(val_fer_features[batch_indices], dtype=torch.float32, device=device)\n",
        "            fer_batch_features = torch.stack([\n",
        "                fer_batch_features[i] if val_face_detected[batch_indices[i]] == 1 else zero_vector_fer\n",
        "                for i in range(len(batch_indices))\n",
        "            ])\n",
        "\n",
        "            # Handle Pose features\n",
        "            pose_batch_indices = list(range(batch_idx * val_loader.batch_size, min((batch_idx + 1) * val_loader.batch_size, len(val_pose_features))))\n",
        "            if len(pose_batch_indices) == 0:\n",
        "                continue\n",
        "            pose_data = torch.tensor([\n",
        "                val_pose_features[i] if i < len(val_pose_features) else zero_vector_pose\n",
        "                for i in pose_batch_indices\n",
        "            ], dtype=torch.float32).to(device)\n",
        "\n",
        "            # Check for mismatched batch sizes\n",
        "            if len(fer_batch_features) != data.size(0) or len(pose_data) != data.size(0):\n",
        "                #print(f\"Skipping batch {batch_idx+1} during validation due to mismatched features.\")\n",
        "                continue\n",
        "\n",
        "            outputs = model(data, fer_batch_features, pose_data)\n",
        "            loss = criterion(outputs, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    epoch_val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}\")\n",
        "    # Create logs dictionary to store metrics\n",
        "    logs = {'loss': epoch_loss, 'accuracy': epoch_acc,\n",
        "            'val_loss': epoch_val_loss, 'val_accuracy': epoch_val_acc}\n",
        "\n",
        "    # Call the custom plotting callback\n",
        "    plot_losses.on_epoch_end(epoch, logs)\n",
        "    # Early stopping\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G31OTvCPVMtM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Prediction and report generation\n",
        "model.eval()\n",
        "all_preds, all_targets = [], []\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(val_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Handle FER features\n",
        "        batch_indices = list(range(batch_idx * val_loader.batch_size, min((batch_idx + 1) * val_loader.batch_size, len(val_fer_features))))\n",
        "        if len(batch_indices) == 0:\n",
        "            continue\n",
        "        fer_batch_features = torch.tensor(val_fer_features[batch_indices], dtype=torch.float32, device=device)\n",
        "        fer_batch_features = torch.stack([\n",
        "            fer_batch_features[i] if val_face_detected[batch_indices[i]] == 1 else zero_vector_fer\n",
        "            for i in range(len(batch_indices))\n",
        "        ])\n",
        "\n",
        "        # Handle Pose features\n",
        "        pose_batch_indices = list(range(batch_idx * val_loader.batch_size, min((batch_idx + 1) * val_loader.batch_size, len(val_pose_features))))\n",
        "        if len(pose_batch_indices) == 0:\n",
        "            continue\n",
        "        pose_data = torch.tensor([\n",
        "            val_pose_features[i] if i < len(val_pose_features) else zero_vector_pose\n",
        "            for i in pose_batch_indices\n",
        "        ], dtype=torch.float32).to(device)\n",
        "\n",
        "        # Check for mismatched batch sizes\n",
        "        if len(fer_batch_features) != data.size(0) or len(pose_data) != data.size(0):\n",
        "            print(f\"Skipping batch {batch_idx+1} during prediction due to mismatched features.\")\n",
        "            continue\n",
        "\n",
        "        outputs = model(data, fer_batch_features, pose_data)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "# Check if predictions and targets were collected\n",
        "if len(all_preds) == 0 or len(all_targets) == 0:\n",
        "    print(\"No predictions or targets collected. Skipping evaluation.\")\n",
        "else:\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(all_targets, all_preds)\n",
        "    precision = precision_score(all_targets, all_preds, average='macro')\n",
        "    recall = recall_score(all_targets, all_preds, average='macro')\n",
        "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Generate classification report\n",
        "    class_names = list(activity_map_AUC.values())\n",
        "    report = classification_report(all_targets, all_preds, target_names=class_names, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    report_df_filtered = report_df.drop('accuracy', errors='ignore')\n",
        "    print(tabulate(report_df_filtered, headers='keys', tablefmt='fancy_grid'))\n",
        "\n",
        "    # Confusion matrix\n",
        "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    display_c_m = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
        "    plt.figure(figsize=(10, 9))\n",
        "    display_c_m.plot(cmap='OrRd', xticks_rotation=25)\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.title('Confusion Matrix', fontsize=24)\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sjeKHTcWYT3i"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
